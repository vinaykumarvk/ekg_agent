# -*- coding: utf-8 -*-
"""kg_vector_response_v2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1RqfOjn1N0I4TKLD-NhiLpv3C2J8UA8hz

# Setup

# connect to drive
"""

from google.colab import drive
drive.mount('/content/drive')

"""## Imports"""

import re, json
from collections import deque
import os
from datetime import datetime
from collections import defaultdict, Counter, deque
from typing import Dict, Any, List, Tuple, Iterable, Set
from concurrent.futures import ThreadPoolExecutor, as_completed
import pickle
import numpy as np
from pathlib import Path

import networkx as nx
from openai import OpenAI

try:
    from rapidfuzz import process, fuzz
    _HAS_RAPIDFUZZ = True
except ImportError:
    _HAS_RAPIDFUZZ = False
    print("Warning: rapidfuzz not available, using fallback")

try:
    from IPython.display import display, Markdown
    _HAS_DISPLAY = True
except ImportError:
    _HAS_DISPLAY = False

try:
    import ipywidgets as widgets
    from IPython.display import display, clear_output
    _HAS_WIDGETS = True
except ImportError:
    _HAS_WIDGETS = False
    print("Warning: ipywidgets not available")

"""## Configuration Presets"""

import os

BASE_FOLDER = '/content/drive/MyDrive/Wealth EKG/Product EKG'
DOC_VECTOR_STORE_ID = os.getenv("DOC_VECTOR_STORE_ID", "vs_6910a0f29b548191befd180730d968ee")
KG_VECTOR_STORE_ID = os.getenv("KG_VECTOR_STORE_ID", "vs_6934751b8a90819190113fe85b689848")
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")  # Must be set in environment
client = OpenAI(api_key=OPENAI_API_KEY)

ANSWER_PRESETS = {
    "concise": {
        "hops": 1,
        "k_suggest": 12,
        "k_candidates": 15,
        "max_chunks": 6,
        "k_each": 2,
        "lambda_div": 0.6,
        "max_tokens": 1500,
        "max_expanded": 40,
        "model": "gpt-5-nano",
        "_mode": "concise"  # ADD THIS
    },
    "balanced": {
        "hops": 1,
        "k_suggest": 20,
        "k_candidates": 25,
        "max_chunks": 10,
        "k_each": 3,
        "lambda_div": 0.6,
        "max_tokens": 6000,
        "max_expanded": 60,
        "model": "gpt-5.1",
        "_mode": "balanced"  # ADD THIS
    },
    "deep": {
        "hops": 2,
        "k_suggest": 35,
        "k_candidates": 40,
        "max_chunks": 22,
        "k_each": 8,
        "lambda_div": 0.75,
        "max_tokens": 20000,
        "max_expanded": 120,
        "model": "o3-deep-research",  # Use gpt-4o since gpt-5 doesn't exist
        "_mode": "deep"  # ADD THIS
    },
        "stepback": {
        "hops": 2,
        "k_suggest": 35,
        "k_candidates": 40,
        "max_chunks": 22,
        "k_each": 8,
        "lambda_div": 0.75,
        "max_tokens": 20000,
        "max_expanded": 120,
        "model": "gpt-5-nano",  # Use gpt-4o since gpt-5 doesn't exist
        "_mode": "deep"  # ADD THIS
    }
}

def get_preset(mode="balanced"):
    mode = mode.lower()
    if mode not in ANSWER_PRESETS:
        print(f"Warning: Unknown mode '{mode}', using 'balanced'")
        mode = "balanced"
    return ANSWER_PRESETS[mode]

"""## prompts"""

file_search_message = """
You are an internal wealth-management product knowledge assistant.

You have access to the following:
1. A structured set of business concepts, entities, and relationships derived from our internal knowledge graph:
<KG_CONTEXT>
{{kg_text}}
</KG_CONTEXT>

2. A document vector store containing detailed product, process, and journey information.

The user has provided several questions and hints that together express ONE underlying information need.
Treat ALL of the following as different perspectives of the same broader inquiry:

<USER_QUESTIONS>
{expanded_queries_str}
</USER_QUESTIONS>

------------------------------------------------------------
YOUR TASK
------------------------------------------------------------

1) **Understand the Unified Intent**
   Infer the single business problem or information objective these questions collectively express.
   Reformulate this as a clear, concise intent statement.

2) **Use the KG Context Internally**
   Use the entities and relationships in <KG_CONTEXT> ONLY for internal reasoning—
   NOT for display, NOT for enumeration, NOT for explanation.
   They exist only to guide your understanding of concepts and their interactions.

3) **Retrieve the Most Relevant Knowledge**
   Internally issue file_search queries into the document vector store by combining:
      • the unified intent
      • the expanded understanding of the topic
      • relevant concepts implied by the KG context
   Do NOT mention file_search, retrieval, vector stores, or any internal mechanisms in your final answer.

4) **Synthesize ONE Integrated Explanation**
   Using the retrieved content:
      • Produce a cohesive, structured narrative that directly answers the unified intent.
      • Merge overlapping points and remove redundancies.
      • Ensure the explanation is generic and platform-agnostic.
      • Do NOT reference clients, banks, proprietary implementations, or example organizations.
      • Do NOT mention knowledge graphs, nodes, edges, triples, metadata, or relationships.
      • Do NOT start the answer with statements like “Below is an integrated view…”,
        “The KG shows…”, “From the knowledge graph…”, or any similar framing.
      • Simply present the explanation directly and professionally, as if preparing internal
        product documentation or briefing an experienced BA.

5) **Final Output Format (Strict)**
Respond ONLY with the following JSON object:

{{{{
  "stepback_intent": "A clear statement of the unified intent.",
  "expanded_question": "A concise, enriched reformulation.",
  "business_entities": ["entity 1", "entity 2", "..."],
  "answer": "A single integrated answer written in **proper, clean Markdown**, with headings, subheadings, bullet points, or numbered lists.\n             The answer must NOT include the KG, nodes, relationships, internal tools, or any meta-commentary."
}}}}

Notes:
- The **answer** field must contain fully valid Markdown **inside the JSON** (no backticks).
- Do NOT add any prose outside the JSON.
- Do NOT confirm the usage of KG or documents.
- Do NOT reference the prompt, instructions, or tools in the answer.
"""

stepback_message = """
You are a wealth-management product knowledge assistant.

You ALWAYS receive one user question, and you MUST analyse it without asking the user to repeat it.

You have access to a file_search tool connected to a KG vector store
containing natural-language descriptions of platform knowledge-graph nodes.

===========================================================
YOUR MANDATORY TASKS
===========================================================

For the given user question (provided in the `original_question` field below):

1) STEP-BACK INTENT
   Rewrite the question in a more generic, clarified formulation that captures
   the underlying business intent. No loss of meaning. No assumptions.

2) EXPANDED QUESTION
   Expand the question into a richer, more detailed version intended to improve
   retrieval from the KG. Keep it concise, business-focused, and not rhetorical.

3) EXTRACT BUSINESS ENTITIES
   Extract key BUSINESS entities mentioned or implied in the expanded question.
   Examples: workflows, screens, documents, validations, client data entities,
   approval processes, reports, regulatory aspects, system modules, etc.
   Output as a list of short canonical phrases.

4) RETRIEVE RELEVANT KG NODES
   - You MUST call the file_search tool.
   - Build your search query using a combination of:
        a) the expanded question
        b) the extracted entities (as keywords)
   - Retrieve up to 10 (not necessarily exactly 10) relevant KG nodes.
   - From each result, extract ONLY the KG node NAME (not description).

5) OUTPUT FORMAT (STRICT)
   You MUST output ONLY the following JSON object:

   {{{{
     "original_question": "...",
     "stepback_question": "...",
     "expanded_question": "...",
     "entities": ["...", "..."],
     "node_names": ["...", "..."]
   }}}}

   - All string fields may contain markdown formatting.
   - "entities" must be an array of strings.
   - "node_names" must be an array of strings. If no matches, return [].

===========================================================
STRICT RULES
===========================================================

- DO NOT ask the user to provide the question again.
- DO NOT output explanations, commentary, or extra text.
- DO NOT refuse unless the input is empty.
- DO NOT repeat tool call results; only extract node names.
- DO NOT invent node names; use only those returned by file_search.

===========================================================
INPUT TO PROCESS
===========================================================

original_question: {question}
"""

formatting_messge = """
You are a senior editor specializing in transforming research reports into
clear, polished, publication-ready documents.

Your task is to refine the following research output:

<RAW_ANSWER>
{{answer}}
</RAW_ANSWER>

------------------------------------------------------------
INSTRUCTIONS
------------------------------------------------------------

1. **Improve Language & Clarity**
   - Rewrite the content to be clearer, sharper, and more professional.
   - Improve flow, transitions, and readability without changing factual meaning.
   - Remove repetition and tighten overly long phrasing.

2. **Improve Structure & Formatting**
   - Apply clean Markdown formatting.
   - Use headings, subheadings, numbered lists, and bullet points where helpful.
   - Ensure the document reads like a polished research brief or executive summary.

3. **Handle References Correctly**
   - Remove all inline references, markers, or citation tags.
   - Collect all references mentioned anywhere in the text.
   - Present them in a **single consolidated "References" section** at the end.
   - If a reference appears multiple times, list it only once.

4. **Preserve Meaning, Remove Noise**
   - Do NOT hallucinate any new facts.
   - Preserve technical accuracy, definitions, and logical structure.
   - Eliminate filler language or unnecessary qualifiers.

5. **Output Format**
   - Return ONLY the refined research report as a **Markdown document**.
   - No extra commentary, no explanation of changes.

"""

PROMPT_SET = {
    "concise": file_search_message,
    "balanced": file_search_message,
    "deep": file_search_message,
    "stepback": stepback_message,
    "formatting": formatting_messge,
}

"""# Initialise functions

## Step 3: Graph Expansion
"""

def expand_nodes(G, seed_ids, hops=1, edge_type_whitelist=None, max_expanded=60, preset_params=None):
    if preset_params:
        max_expanded = preset_params.get("max_expanded", max_expanded)

    def _ok_edge(edata):
        if not edge_type_whitelist:
            return True
        et = edata.get("type") or edata.get("label") or "RELATED"
        return et in edge_type_whitelist

    seen = set(seed_ids)
    q = deque([(nid, 0) for nid in seed_ids])
    edges = []

    while q and len(seen) < max_expanded:
        u, d = q.popleft()
        if u not in G:
            continue

        for v in G[u]:
            for _, edata in G[u][v].items():
                if not _ok_edge(edata):
                    continue
                et = edata.get("type") or edata.get("label") or "RELATED"
                edge = {"source_id": u, "target_id": v, "type": str(et)}
                if "evidence" in edata:
                    edge["evidence"] = edata["evidence"]
                if "source_documents" in edata:
                    edge["source_documents"] = edata["source_documents"]
                edges.append(edge)
            if v not in seen and d < hops:
                seen.add(v)
                q.append((v, d + 1))

        if hasattr(G, "predecessors"):
            for v in G.predecessors(u):
                for _, edata in G[v][u].items():
                    if not _ok_edge(edata):
                        continue
                    et = edata.get("type") or edata.get("label") or "RELATED"
                    edge = {"source_id": v, "target_id": u, "type": str(et)}
                    if "evidence" in edata:
                        edge["evidence"] = edata["evidence"]
                    if "source_documents" in edata:
                        edge["source_documents"] = edata["source_documents"]
                    edges.append(edge)
                if v not in seen and d < hops:
                    seen.add(v)
                    q.append((v, d + 1))

    edges = _dedup(edges, key=lambda e: (e["source_id"], e["target_id"], e["type"]))
    return list(seen), edges

"""## Complete KG Pipeline"""

def answer_with_kg(q, G, by_id, name_index, llm_client, hops=1, k_suggest=20, k_candidates=25, preset_params=None):
    if preset_params:
        hops = preset_params.get("hops", hops)
        k_suggest = preset_params.get("k_suggest", k_suggest)
        k_candidates = preset_params.get("k_candidates", k_candidates)

    suggested = stepback_entity_candidates(q, llm_client, k_suggest=k_suggest)
    matched = match_entities_to_graph(suggested, name_index, by_id, k_candidates=k_candidates)
    seed_ids = [nid for nid, _ in matched]

    if not seed_ids:
        return {
            "answer": "_No entities matched in the knowledge graph._",
            "resolved_entities": [],
            "supporting_edges": [],
            "grounding": {},
            "meta": {"reason": "No seeds", "suggested": suggested}
        }

    expanded_nodes, expanded_edges = expand_nodes(G, seed_ids, hops=hops, max_expanded=60, preset_params=preset_params)
    data = grounded_answer_llm(q, expanded_nodes, expanded_edges, by_id, llm_client, preset_params=preset_params)

    data["meta"].update({
        "suggested_entities": suggested,
        "seed_ids": seed_ids,
        "expanded_nodes": len(expanded_nodes),
        "expanded_edges": len(expanded_edges)
    })
    data["resolved_entities"] = expanded_nodes
    data["supporting_edges"] = expanded_edges
    return data

"""# Load KG

## Load Knowledge Graph
"""

with open(os.path.join(BASE_FOLDER, "master_knowledge_graph.json"), "r") as f:
    KG = json.load(f)

nodes = KG.get("nodes", [])
edges = KG.get("edges", [])
print(f"Loaded {len(nodes)} nodes and {len(edges)} edges")

"""## Build Indexes"""

def _norm(s):
    return re.sub(r"\s+", " ", (s or "").strip().lower())

by_id = {}
name_index = defaultdict(list)

for n in nodes:
    nid = n["id"]
    by_id[nid] = n
    names = [n.get("name", "")] + n.get("aliases", [])
    for t in names:
        if t:
            name_index[_norm(t)].append(nid)

print(f"Indexed {len(by_id)} nodes, {len(name_index)} names/aliases")

"""## Build NetworkX Graph"""

G = nx.MultiDiGraph()
for n in nodes:
    G.add_node(n["id"], **n)

for e in edges:
    u, v = e["source_id"], e["target_id"]
    attrs = e.get("properties", {}).copy()
    attrs["type"] = e.get("type")
    G.add_edge(u, v, **attrs)

print(f"Graph: {G.number_of_nodes()} nodes, {G.number_of_edges()} edges")

"""#Common functions"""

def get_response(message, client, model, vector_ids, stream=False):
  response = client.responses.create(
      model=model,
      input=[
          {
              "role": "user",
              "content": message
          }
      ],
      tools=[
          {
              "type": "file_search",
              "vector_store_ids": vector_ids
          }
      ],
      stream = stream
  )
  return response

def parse_llm_json(text):
    # 1. Strip markdown fences if present
    cleaned_text = text.strip()
    if cleaned_text.startswith("```json"):
        cleaned_text = cleaned_text[len("```json"):].strip()
    if cleaned_text.endswith("```"):
        cleaned_text = cleaned_text[:-len("```")].strip()

    try:
        return json.loads(cleaned_text)
    except json.JSONDecodeError as e:
        print(f"Initial JSONDecodeError: {e}. Attempting further cleanup.")
        # 2. Try to remove invalid control characters (0x00-0x1F, 0x7F) that are not allowed in JSON strings
        # This will remove characters like vertical tab (\x0b), form feed (\x0c) etc.
        cleaned_text_stage2 = re.sub(r'[\x00-\x1f\x7f]', '', cleaned_text)
        try:
            return json.loads(cleaned_text_stage2)
        except json.JSONDecodeError as e2:
            print(f"Second JSONDecodeError after control character removal: {e2}. Could not parse LLM response.")
            raise

"""# step-1: Stepback thinking and Extract relevant entities from KG text vector"""

# ==============================================================================
# STEP 1: Get Node Names from GPT with File Search
# ==============================================================================

def get_relevant_nodes(
    question: str,
    kg_vector_store_id: str,
    client,
    PROMPT_SET,
    ANSWER_PRESETS,
    vector_ids,
    max_nodes: int = 10
) -> Dict[str, Any]:
    """
    Use GPT with file_search to get relevant nodes, entities, and stepback.

    Returns:
        {
            "original_question": "...",
            "stepback_question": "...",
            "expanded_question": "...",
            "entities": ["...", "..."],
            "node_names": ["...", "..."]
        }
    """

    message = PROMPT_SET["stepback"].format(question=question)


    try:
        resp = get_response(message, client, model, vector_ids)
    except Exception as e:
        print(f"Error in response from openAI - get_relevant_nodes_with_context: {e}")
        return

    # Parse response
    output_text = resp.output_text if hasattr(resp, 'output_text') else resp.content[0].text

    # Clean and parse JSON
    output_text = output_text.replace("```json", "").replace("```", "").strip()

    try:
        result = json.loads(output_text)
        return result
    except json.JSONDecodeError as e:
        print(f"Warning: Could not parse JSON: {e}")
        print(f"Raw output: {output_text[:500]}")

        # Fallback: return minimal structure
        return {
            "original_question": question,
            "stepback_question": question,
            "expanded_question": question,
            "entities": [],
            "node_names": []
        }

"""# step-2: Find related nodes"""

# ==============================================================================
# STEP 2: Map Node Names to IDs (New Helper Function)
# ==============================================================================

def map_node_names_to_ids(
    node_names: List[str],
    by_id: Dict[str, Dict],
    name_index: Dict[str, Any] = None
) -> List[str]:
    """
    Map node names returned by GPT to actual node IDs in the graph.

    Args:
        node_names: List of node names from GPT (e.g., ["customer card", "product selection screen"])
        by_id: Your existing node lookup dict {node_id: node_data}
        name_index: Optional name-to-ID index for faster lookup

    Returns:
        List of node IDs that exist in the graph
    """

    node_ids = []
    not_found = []

    for node_name in node_names:
        node_name_lower = node_name.lower().strip()

        # Strategy 1: Use name_index if available (fastest)
        if name_index:
            # Try exact match
            if node_name_lower in name_index:
                ids = name_index[node_name_lower]
                if isinstance(ids, (list, tuple)):
                    node_ids.extend([str(id) for id in ids])
                else:
                    node_ids.append(str(ids))
                continue

        # Strategy 2: Search through by_id (fallback)
        found = False
        for node_id, node_data in by_id.items():
            # Get node name from data
            actual_name = (
                node_data.get('name') or
                node_data.get('node_name') or
                node_data.get('label') or
                str(node_id)
            ).lower().strip()

            # Check for match
            if node_name_lower == actual_name or node_name_lower in actual_name or actual_name in node_name_lower:
                node_ids.append(str(node_id))
                found = True
                break

        if not found:
            not_found.append(node_name)

    # Report not found nodes
    if not_found:
        print(f"⚠️  Could not find {len(not_found)} nodes in graph:")
        for name in not_found:
            print(f"   - {name}")

    # Deduplicate
    return list(set(node_ids))


# ==============================================================================
# STEP 3: Graph Expansion (Your Existing Function - Keep As Is)
# ==============================================================================

def expand_nodes(G, seed_ids, hops=1, edge_type_whitelist=None, max_expanded=60, preset_params=None):
    """
    Your existing expand_nodes function - NO CHANGES NEEDED!

    This stays exactly the same.
    """
    if preset_params:
        max_expanded = preset_params.get("max_expanded", max_expanded)

    def _ok_edge(edata):
        if not edge_type_whitelist:
            return True
        et = edata.get("type") or edata.get("label") or "RELATED"
        return et in edge_type_whitelist

    seen = set(seed_ids)
    q = deque([(nid, 0) for nid in seed_ids])
    edges = []

    while q and len(seen) < max_expanded:
        u, d = q.popleft()
        if u not in G:
            continue

        for v in G[u]:
            for _, edata in G[u][v].items():
                if not _ok_edge(edata):
                    continue
                et = edata.get("type") or edata.get("label") or "RELATED"
                edge = {"source_id": u, "target_id": v, "type": str(et)}
                if "evidence" in edata:
                    edge["evidence"] = edata["evidence"]
                if "source_documents" in edata:
                    edge["source_documents"] = edata["source_documents"]
                edges.append(edge)
            if v not in seen and d < hops:
                seen.add(v)
                q.append((v, d + 1))

        if hasattr(G, "predecessors"):
            for v in G.predecessors(u):
                for _, edata in G[v][u].items():
                    if not _ok_edge(edata):
                        continue
                    et = edata.get("type") or edata.get("label") or "RELATED"
                    edge = {"source_id": v, "target_id": u, "type": str(et)}
                    if "evidence" in edata:
                        edge["evidence"] = edata["evidence"]
                    if "source_documents" in edata:
                        edge["source_documents"] = edata["source_documents"]
                    edges.append(edge)
                if v not in seen and d < hops:
                    seen.add(v)
                    q.append((v, d + 1))

    edges = list(set([
        (e["source_id"], e["target_id"], e["type"])
        for e in edges
    ]))
    edges = [
        {"source_id": s, "target_id": t, "type": typ}
        for s, t, typ in edges
    ]

    return list(seen), edges


# ==============================================================================
# COMPLETE NEW WORKFLOW
# ==============================================================================

def get_relevant_subgraph(
    question: str,
    G,  # Your NetworkX graph
    by_id: Dict[str, Dict],  # Your node lookup
    kg_vector_store_id: str,
    client,
    stepback_response:Dict, # Moved here
    name_index: Dict[str, Any] = None,
    edge_type_whitelist: List[str] = None,
    hops: int = 1,
    max_expanded: int = 60
) -> Dict[str, Any]:
    """
    Complete new workflow: GPT file search + graph expansion.

    Replaces the old 4-step process.

    Returns:
        {
            "question_analysis": {...},  # Stepback, entities, etc.
            "seed_node_ids": [...],       # Starting nodes
            "expanded_node_ids": [...],   # All nodes in subgraph
            "edges": [...],               # Edges in subgraph
            "nodes": {}                # Node data for subgraph
        }
    """

    # print("=" * 80)
    # print("GETTING RELEVANT SUBGRAPH (New Workflow)")
    # print("=" * 80)

    # # STEP 1: Get node names from GPT with file_search
    # print("\nStep 1: GPT with file_search...")
    analysis = stepback_response
    # print(f"  Stepback: {analysis['stepback_question']}")
    # print(f"  Entities: {analysis['entities']}")
    # print(f"  Node names: {analysis['node_names']}")

    # STEP 2: Map node names to IDs
    # print("\nStep 2: Mapping node names to IDs...")
    seed_node_ids = map_node_names_to_ids(
        node_names=analysis['node_names'],
        by_id=by_id,
        name_index=name_index
    )

    # print(f"  Found {len(seed_node_ids)} seed nodes in graph")

    if not seed_node_ids:
        # print("  ⚠️  No seed nodes found!")
        return {
            "question_analysis": analysis,
            "seed_node_ids": [],
            "expanded_node_ids": [],
            "edges": [],
            "nodes": {}
        }

    # STEP 3: Expand graph from seed nodes
    # print(f"\nStep 3: Expanding graph ({hops} hops)...")
    expanded_node_ids, edges = expand_nodes(
        G=G,
        seed_ids=seed_node_ids,
        hops=hops,
        edge_type_whitelist=edge_type_whitelist,
        max_expanded=max_expanded
    )

    # print(f"  Expanded to {len(expanded_node_ids)} total nodes")
    # print(f"  Found {len(edges)} edges")

    # STEP 4: Get node data for expanded nodes
    nodes = {}
    for node_id in expanded_node_ids:
        if node_id in by_id:
            nodes[node_id] = by_id[node_id]

    # print(f"\n✓ Complete! Subgraph ready.")
    # print("=" * 80)

    return {
        "question_analysis": analysis,
        "seed_node_ids": seed_node_ids,
        "expanded_node_ids": expanded_node_ids,
        "edges": edges,
        "nodes": nodes
    }

"""# Step-3: Get graph guided queries"""

def build_kg_guided_queries(
    question: str,
    kg_result: Dict[str, Any],  # Takes whole result
    by_id: Dict[str, Dict],
    max_queries: int = 12
) -> List[str]:
    """Build KG-guided queries for vector retrieval."""

    queries = []

    # Get data from kg_result
    expanded_node_ids = kg_result["expanded_node_ids"]
    edges = kg_result["edges"]
    analysis = kg_result["question_analysis"]

    # 1. Original question
    queries.append(question)

    # 2. Stepback/expanded
    if "stepback_question" in analysis:
        queries.append(analysis["stepback_question"])
    if "expanded_question" in analysis:
        queries.append(analysis["expanded_question"])

    # 3. Entity-focused
    node_names = []
    for node_id in expanded_node_ids[:8]:
        if node_id in by_id:
            name = by_id[node_id].get('name', by_id[node_id].get('node_name', str(node_id)))
            node_names.append(name)

    for entity in node_names[:5]:
        queries.append(f"{question} {entity}")

    # 4. Relationship-focused
    node_lookup = {}
    for node_id in expanded_node_ids:
        if node_id in by_id:
            name = by_id[node_id].get('name', by_id[node_id].get('node_name', str(node_id)))
            node_lookup[node_id] = name

    for edge in edges[:10]:
        source_name = node_lookup.get(edge["source_id"], "")
        target_name = node_lookup.get(edge["target_id"], "")
        edge_type = edge["type"]

        if source_name and target_name:
            queries.append(f"{source_name} {edge_type} {target_name}")

    # 5. Deduplicate
    seen = set()
    unique_queries = []
    for q in queries:
        q_lower = q.lower()
        if q_lower not in seen:
            seen.add(q_lower)
            unique_queries.append(q)

    return unique_queries[:max_queries]

"""# Step-4: Create nodes and relationship for enriching vector search"""

def generate_kg_text(
    kg_result: Dict[str, Any],
    by_id: Dict[str, Dict],
):

    expanded_node_ids = kg_result["expanded_node_ids"]
    edges = kg_result["edges"]
    analysis = kg_result["question_analysis"]

    # Build compact node list
    nodes_summary = []
    for node_id in expanded_node_ids[:40]:  # Limit to 40 nodes
        if node_id in by_id:
            name = by_id[node_id].get('name', by_id[node_id].get('node_name', str(node_id)))
            node_type = by_id[node_id].get('node_type', by_id[node_id].get('type', 'Entity'))
            nodes_summary.append(f"• {name} ({node_type})")

    # Build compact edge list
    node_lookup = {}
    for node_id in expanded_node_ids:
        if node_id in by_id:
            name = by_id[node_id].get('name', by_id[node_id].get('node_name', str(node_id)))
            node_lookup[node_id] = name

    edges_summary = []
    for edge in edges[:50]:  # Limit to 50 edges
        source = node_lookup.get(edge["source_id"], "?")
        target = node_lookup.get(edge["target_id"], "?")
        rel_type = edge.get("type", "RELATED")
        edges_summary.append(f"• {source} --[{rel_type}]→ {target}")

    kg_structure_text = f"""
    The following entities and relationships are relevant to your question:

    ENTITIES:
    {chr(10).join(nodes_summary)}

    RELATIONSHIPS:
    {chr(10).join(edges_summary)}

    This structure shows WHAT entities exist and HOW they relate.
    Use this to understand the architecture and connections.
    The actual detailed documentation will be retrieved via file_search.
    """
    return kg_structure_text

"""# Get response"""

mode = "concise"
question = "When is OTP required to be used?"

import time
time1 = time.time()
stepback_response = get_relevant_nodes(
  question=question,
  kg_vector_store_id=KG_VECTOR_STORE_ID,
  client=client,
  PROMPT_SET=PROMPT_SET,
  ANSWER_PRESETS=ANSWER_PRESETS,
  vector_ids=[KG_VECTOR_STORE_ID],
  max_nodes=10
  )

print(f'stepback: {time.time() - time1} sec')

expanded_nodes = get_relevant_subgraph(
        question=question,
        G=G,
        by_id=by_id,
        kg_vector_store_id=KG_VECTOR_STORE_ID,
        client=client,
        name_index=name_index,  # Optional, can be None
        hops=1,
        max_expanded=60,
        stepback_response=stepback_response
    )

# NEW: Build queries using KG
expanded_queries = build_kg_guided_queries(
    question=question,
    kg_result=expanded_nodes,
    by_id=by_id,
    max_queries=12
)

kg_text = generate_kg_text(expanded_nodes, by_id)

model = ANSWER_PRESETS[mode]['model']
expanded_queries_str = chr(10).join(f"{i+1}. {q}" for i, q in enumerate(expanded_queries))
message = PROMPT_SET['concise'].format(
    expanded_queries_str=expanded_queries_str,
    kg_text=kg_text)
vector_ids=[DOC_VECTOR_STORE_ID]

resp = get_response(
    message=message,
    client=client,
    model=model,
    vector_ids=vector_ids
)
print(f'total time: {time.time()-time1} sec')
# answer = stream_response(message, client, model, vector_ids)

# Apply the robust JSON parsing function
# answer_json = parse_llm_json(resp.output_text)
# answer = answer_json['answer']
display(Markdown(message))

display.markdown(json.loads(resp.output_text)['answer'])

# Apply the robust JSON parsing function
answer_json = parse_llm_json(resp.output_text)
answer = answer_json['answer']
display(Markdown(answer))

def stream_response(message, client, model, vector_ids):
    """
    Stream a response from the Responses API including tool calls.
    Handles:
      - output_text.delta
      - tool_call events
      - tool_result events
    """

    # Always stream here – this function is for streaming usage
    iterator = get_response(message, client, model, vector_ids, stream=False)

    full_text = ""

    for event in iterator:
        # Safely convert event to dict
        data = event.model_dump()
        etype = data.get("type", "")

        # -----------------------------
        # 1. TEXT CHUNKS (main streaming)
        # -----------------------------
        if "output_text.delta" in etype:
            delta = data.get("delta", "")
            if isinstance(delta, dict):
                chunk = delta.get("text", "")
            elif isinstance(delta, str):
                chunk = delta
            else:
                chunk = ""

            if chunk:
                print(chunk, end="", flush=True)
                full_text += chunk

        # -----------------------------
        # 2. TOOL CALL START
        # -----------------------------
        elif "tool_call" in etype:
            # Optional: print something or ignore
            tool_type = data.get("tool", {}).get("type")
            # You can comment this out if you don’t want to see tool events
            # print(f"\n[Tool call starting: {tool_type}]\n", flush=True)
            pass

        # -----------------------------
        # 3. TOOL RESULTS RETURNING
        # -----------------------------
        elif "tool_result" in etype:
            # Optional: hide, print, or log tool results
            # print("\n[Tool result received]\n", flush=True)
            pass

        # -----------------------------
        # 4. COMPLETION EVENT
        # -----------------------------
        elif "completed" in etype:
            # Optional: final marker
            # print("\n[Response completed]\n", flush=True)
            pass

    print()  # newline at end
    return full_text