# -*- coding: utf-8 -*-
"""kg_vector_response_v1.ipynb

Automatically generated by Colab.

Original file is located at

# Setup

## Setup: Mount Drive and Install Dependencies
"""



"""## Imports"""

try:
    import gradio as gr
    _HAS_GRADIO = True
except ImportError:
    _HAS_GRADIO = False
    gr = None

import html
import re, json
import threading

import json
import re
import os
from datetime import datetime
from collections import defaultdict, Counter, deque
from typing import Dict, Any, List, Tuple, Iterable, Set
from concurrent.futures import ThreadPoolExecutor, as_completed
import pickle
import numpy as np
from pathlib import Path

import networkx as nx
from openai import OpenAI

try:
    from rapidfuzz import process, fuzz
    _HAS_RAPIDFUZZ = True
except ImportError:
    _HAS_RAPIDFUZZ = False
    import logging
    log = logging.getLogger(__name__)
    log.warning("rapidfuzz not available, using fallback")

try:
    from IPython.display import display, Markdown
    _HAS_DISPLAY = True
except ImportError:
    _HAS_DISPLAY = False

try:
    import ipywidgets as widgets
    from IPython.display import display, clear_output
    _HAS_WIDGETS = True
except ImportError:
    _HAS_WIDGETS = False
    import logging
    log = logging.getLogger(__name__)
    log.warning("ipywidgets not available")

"""## Answer Caching System"""

# Cache configuration - Production ready
import time
from collections import OrderedDict
from typing import Optional

# Use settings for cache configuration
try:
    from api.settings import settings
    CACHE_DIR = settings.CACHE_DIR
    MAX_CACHE_SIZE = settings.MAX_CACHE_SIZE
    CACHE_TTL = settings.CACHE_TTL
except ImportError:
    # Fallback if settings not available
    CACHE_DIR = os.getenv("CACHE_DIR", "/tmp/ekg_cache")
    MAX_CACHE_SIZE = int(os.getenv("MAX_CACHE_SIZE", "1000"))
    CACHE_TTL = int(os.getenv("CACHE_TTL", "3600"))

os.makedirs(CACHE_DIR, exist_ok=True)
ANSWER_CACHE_FILE = os.path.join(CACHE_DIR, "answer_cache.pkl")
SIMILARITY_THRESHOLD = 0.80

class LRUCache:
    """Thread-safe LRU cache with TTL support"""
    def __init__(self, max_size: int = None, ttl: int = None):
        self.max_size = max_size or MAX_CACHE_SIZE
        self.ttl = ttl or CACHE_TTL
        self.cache = OrderedDict()
        self._lock = threading.Lock()
    
    def get(self, key: str) -> Optional[Any]:
        with self._lock:
            if key in self.cache:
                value, timestamp = self.cache[key]
                if time.time() - timestamp < self.ttl:
                    self.cache.move_to_end(key)
                    return value
                else:
                    del self.cache[key]
            return None
    
    def set(self, key: str, value: Any) -> None:
        with self._lock:
            if len(self.cache) >= self.max_size:
                self.cache.popitem(last=False)
            self.cache[key] = (value, time.time())
    
    def clear(self) -> None:
        with self._lock:
            self.cache.clear()
    
    def size(self) -> int:
        with self._lock:
            return len(self.cache)

def get_question_embedding(client, question):
    """Get embedding for semantic similarity"""
    response = client.embeddings.create(
        model="text-embedding-3-small",
        input=question
    )
    return np.array(response.data[0].embedding)

def cosine_similarity(a, b):
    """Calculate cosine similarity between two vectors"""
    return np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b))

def load_answer_cache():
    """Load cached answers from disk"""
    if Path(ANSWER_CACHE_FILE).exists():
        with open(ANSWER_CACHE_FILE, 'rb') as f:
            cache = pickle.load(f)
        print(f"✓ Loaded {len(cache)} cached answers")
        return cache
    return {}

def save_answer_cache(answer_cache):
    """Save answer cache to disk"""
    Path(CACHE_DIR).mkdir(parents=True, exist_ok=True)
    with open(ANSWER_CACHE_FILE, 'wb') as f:
        pickle.dump(answer_cache, f)
    print(f"✓ Saved {len(answer_cache)} answers to cache")

def find_similar_cached_answer(client, question, mode, answer_cache, threshold=SIMILARITY_THRESHOLD):
    """
    Find cached answer for semantically similar question with same mode
    Returns: (cache_key, cached_data, similarity_score) or (None, None, 0)
    """
    if not answer_cache:
        return None, None, 0

    query_embedding = get_question_embedding(client, question)

    best_match = None
    best_similarity = 0
    best_key = None

    for cache_key, cached_data in answer_cache.items():
        # Must match mode exactly
        if cached_data.get("mode") != mode:
            continue

        cached_embedding = cached_data.get("embedding")
        if cached_embedding is None:
            continue

        # Convert list back to numpy array
        cached_embedding = np.array(cached_embedding)
        similarity = cosine_similarity(query_embedding, cached_embedding)

        if similarity > best_similarity:
            best_similarity = similarity
            best_match = cached_data
            best_key = cache_key

    if best_similarity >= threshold:
        return best_key, best_match, best_similarity

    return None, None, best_similarity

def cache_answer(answer_cache, question, mode, result, embedding, client):
    """Store answer with metadata and embedding"""
    import hashlib
    cache_key = hashlib.md5(f"{question}|{mode}".encode()).hexdigest()

    answer_cache[cache_key] = {
        "question": question,
        "mode": mode,
        "result": result,
        "embedding": embedding.tolist(),  # Convert numpy to list for pickle
        "timestamp": datetime.now().isoformat(),
        "model": result.get("model_used", "gpt-4o")
    }

    # Save immediately after adding
    save_answer_cache(answer_cache)
    return cache_key

"""## Cache Management Utilities"""

def view_cache_contents():
    """Display all cached questions"""
    cache = load_answer_cache()
    if not cache:
        print("Cache is empty")
        return

    print(f"\nCached Answers ({len(cache)} total):")
    print("="*80)
    for i, (key, data) in enumerate(cache.items(), 1):
        print(f"{i}. [{data['mode'].upper()}] {data['question']}")
        print(f"   Cached: {data['timestamp']}")
        print(f"   Model: {data.get('model', 'unknown')}")
        print()

def clear_answer_cache():
    """Clear all cached answers"""
    if Path(ANSWER_CACHE_FILE).exists():
        os.remove(ANSWER_CACHE_FILE)
        print("✓ Answer cache cleared")
    else:
        print("Cache file doesn't exist")

# Uncomment to use:
# view_cache_contents()
# clear_answer_cache()

def clear_retrieval_cache():
    """Clear the query and hits caches for vector store retrieval"""
    global _Q_CACHE, _HITS_CACHE
    _Q_CACHE.clear()
    _HITS_CACHE.clear()
    import logging
    log = logging.getLogger(__name__)
    log.info("Retrieval cache cleared")
    log.info(f"Q_CACHE size: {_Q_CACHE.size()}, HITS_CACHE size: {_HITS_CACHE.size()}")

"""## Configuration Presets"""

ANSWER_PRESETS = {
    "concise": {
        "hops": 1,
        "k_suggest": 12,
        "k_candidates": 15,
        "max_chunks": 6,
        "k_each": 2,
        "lambda_div": 0.6,
        "max_tokens": 1500,
        "max_expanded": 40,
        "model": "gpt-5-nano",
        "_mode": "concise"
    },
    "balanced": {
        "hops": 1,
        "k_suggest": 20,
        "k_candidates": 25,
        "max_chunks": 10,
        "k_each": 3,
        "lambda_div": 0.6,
        "max_tokens": 6000,
        "max_expanded": 60,
        "model": "gpt-5-mini",
        "_mode": "balanced"
    },
    "deep": {
        "hops": 2,
        "k_suggest": 35,
        "k_candidates": 40,
        "max_chunks": 22,
        "k_each": 8,
        "lambda_div": 0.75,
        "max_tokens": 20000,
        "max_expanded": 120,
        "model": "gpt-5",
        "_mode": "deep"
    }
}

def get_preset(mode="balanced"):
    mode = mode.lower()
    if mode not in ANSWER_PRESETS:
        print(f"Warning: Unknown mode '{mode}', using 'balanced'")
        mode = "balanced"
    return ANSWER_PRESETS[mode]

"""# Initialise functions

## Utility Functions
"""

def _norm(s):
    return re.sub(r"\s+", " ", (s or "").strip().lower())

def _dedup(seq, key=lambda x: x):
    seen, out = set(), []
    for x in seq:
        k = key(x)
        if k not in seen:
            seen.add(k)
            out.append(x)
    return out

def _extract_json_block(text):
    m = re.search(r"```json\s*(\{.*?\})\s*```", text, flags=re.S)
    if m:
        return m.group(1)
    s, e = text.find("{"), text.rfind("}")
    if s != -1 and e != -1 and e > s:
        cand = text[s:e+1]
        if cand.count("{") == cand.count("}"):
            return cand
    return None

def safe_parse_json(block: str):
    """Extract and parse the first JSON object from model output safely."""
    s = block.strip()
    s = re.sub(r"^```(?:json)?\s*|\s*```$", "", s)   # remove ```json fences
    m = re.search(r"\{.*\}", s, flags=re.S)           # find JSON object
    if not m:
        return {}
    try:
        return json.loads(m.group(0))
    except json.JSONDecodeError:
        return {}

def get_output_text(resp):
    if hasattr(resp, "output_text") and isinstance(resp.output_text, str):
        return resp.output_text
    try:
        parts = []
        for item in getattr(resp, "output", []) or []:
            content = getattr(item, "content", None) or item.get("content", None)
            if not content:
                continue
            for seg in content:
                txt = getattr(seg, "text", None) or seg.get("text", None)
                if txt:
                    parts.append(txt)
        if parts:
            return "\n".join(parts)
    except Exception:
        pass
    return str(resp) if resp is not None else ""

def _slugify(text, max_len=80):
    text = (text or "answer").strip().lower()
    text = re.sub(r"[^\w\s-]", "", text)
    text = re.sub(r"\s+", "-", text).strip("-")
    return (text or "answer")[:max_len]

def _short_filename(name, maxlen=64):
    if not name:
        return "unknown-file"
    base = os.path.basename(name)
    return (base[:maxlen-1] + "...") if len(base) > maxlen else base

def normalize_questions(qs):
    """Ensure list of {question:str, options:list[str], correct:int} (max 5)."""
    good = []
    for q in (qs or []):
        if not isinstance(q, dict):
            continue
        if not all(k in q for k in ("question","options","correct")):
            continue
        opts = [str(o).strip() for o in (q.get("options") or [])]
        if len(opts) < 2 or not isinstance(q.get("correct"), int):
            continue
        good.append({
            "question": str(q["question"]).strip(),
            "options": opts,
            "correct": q["correct"]
        })
    return good[:5]

def indent(text, pad="  "):
    """Indent text block"""
    return "\n".join(pad + ln for ln in (text or "").splitlines())

def analyze_answer_formatting(answer_text, mode="balanced"):
    """
    Analyze and report formatting metrics for evaluation scores.
    This helps verify that enhancements are working correctly.

    Args:
        answer_text: The answer to analyze
        mode: The mode used (for comparison with expectations)

    Returns:
        Dictionary with counts and estimated scores
    """
    import re

    if not answer_text:
        print("No answer text to analyze")
        return None

    # Count formatting elements
    citations = len(re.findall(r'\[(?:\d+|KG:[^\]]+)\]', answer_text))

    bullets = answer_text.count('•') + answer_text.count('\n- ') + answer_text.count('\n* ')

    markers = len(re.findall(
        r'(?:Key Point|Note|Important|Example|Reasoning|Evidence|Analysis|Insight|Key Insight|Overview|Summary):',
        answer_text
    ))

    # Count colons for explainability
    colons = answer_text.count(':')

    # Calculate sentence stats
    sentences = len(re.findall(r'[.!?]+\s', answer_text))
    words = len(answer_text.split())
    avg_sentence_length = words / sentences if sentences > 0 else 0

    # Calculate estimated scores using the evaluation formulas
    # Attribution = 3 + (2/1.2) * citation_count, clipped to 1-5
    attribution = min(5.0, max(1.0, 3 + (2/1.2) * citations))

    # Explainability = 3 + 0.25 * markers + 0.3 * bullets, clipped to 1-5
    explainability = min(5.0, max(1.0, 3 + 0.25 * markers + 0.3 * bullets))

    # Clarity = 4 - 0.05 * max(0, mean_words - 30) + 0.2 * bullets, clipped to 1-5
    clarity = 4 - 0.05 * max(0, avg_sentence_length - 30) + 0.2 * bullets
    clarity = min(5.0, max(1.0, clarity))

    # Mode-specific targets
    targets = {
        "concise": {"citations": "1-2", "bullets": "3-5", "markers": "1-2"},
        "balanced": {"citations": "2-3", "bullets": "5-10", "markers": "3-5"},
        "deep": {"citations": "3-5", "bullets": "8-15", "markers": "5-8"}
    }
    target = targets.get(mode, targets["balanced"])

    # Print analysis
    print(f"\n{'='*70}")
    print(f"ANSWER FORMATTING ANALYSIS - Mode: {mode.upper()}")
    print(f"{'='*70}")

    print(f"\nCOUNTS:")
    print(f"  Citations:     {citations:2d}  (target: {target['citations']})")
    print(f"  Bullets:       {bullets:2d}  (target: {target['bullets']})")
    print(f"  Markers:       {markers:2d}  (target: {target['markers']})")
    print(f"  Colons:        {colons:2d}")

    print(f"\nCLARITY METRICS:")
    print(f"  Total words:   {words}")
    print(f"  Sentences:     {sentences}")
    print(f"  Avg words/sentence: {avg_sentence_length:.1f} (target: <30)")

    print(f"\nESTIMATED SCORES:")
    print(f"  Attribution:    {attribution:.2f} / 5.00")
    print(f"  Explainability: {explainability:.2f} / 5.00")
    print(f"  Clarity:        {clarity:.2f} / 5.00")

    # Overall assessment
    avg_score = (attribution + explainability + clarity) / 3
    print(f"\n  Average:        {avg_score:.2f} / 5.00")

    if avg_score >= 4.5:
        print(f"  Assessment:     ✓ EXCELLENT")
    elif avg_score >= 4.0:
        print(f"  Assessment:     ✓ GOOD")
    elif avg_score >= 3.5:
        print(f"  Assessment:     ~ ADEQUATE")
    else:
        print(f"  Assessment:     ⚠ NEEDS IMPROVEMENT")

    print(f"{'='*70}\n")

    return {
        "counts": {
            "citations": citations,
            "bullets": bullets,
            "markers": markers,
            "colons": colons,
            "sentences": sentences,
            "words": words
        },
        "metrics": {
            "avg_sentence_length": avg_sentence_length
        },
        "scores": {
            "attribution": attribution,
            "explainability": explainability,
            "clarity": clarity,
            "average": avg_score
        }
    }

"""## Interactive Answer Review"""

def show_answer_with_review(question, mode, hybrid_result, answer_cache, client):
    """Display answer with OK/Refresh buttons"""

    # Display the answer
    md_text, file_path = export_markdown(final=hybrid_result, question=question)

    if not _HAS_WIDGETS:
        print("\nInteractive buttons not available in this environment")
        return

    # Create buttons
    ok_button = widgets.Button(description="OK", button_style='success', icon='check')
    refresh_button = widgets.Button(description="Refresh Answer", button_style='warning', icon='refresh')
    output = widgets.Output()

    def on_ok_clicked(b):
        with output:
            clear_output()
            print("✓ Answer accepted. Moving on...")

    def on_refresh_clicked(b):
        with output:
            clear_output()
            print("♻️  Regenerating answer with fresh data...\n")

            # Clear retrieval cache and regenerate
            clear_retrieval_cache()
            params = get_preset(mode)

            kg_result = answer_with_kg(
                q=question, G=G, by_id=by_id, name_index=name_index,
                llm_client=client, preset_params=params
            )

            new_hybrid_result = hybrid_answer(
                q=question, kg_result=kg_result, by_id=by_id,
                client=client, vs_id=VECTOR_STORE_ID, preset_params=params
            )

            # Cache the new answer
            query_embedding = get_question_embedding(client, question)
            cache_answer(answer_cache, question, mode, new_hybrid_result, query_embedding, client)

            print("\n✓ Fresh answer generated and cached\n")

            # Display new answer
            md_text, file_path = export_markdown(final=new_hybrid_result, question=question)
            print(f"\nExport completed: {file_path}")

    ok_button.on_click(on_ok_clicked)
    refresh_button.on_click(on_refresh_clicked)

    # Display buttons and output
    print("\n" + "="*80)
    print("Review the answer above. Choose an option:")
    print("="*80)
    button_box = widgets.HBox([ok_button, refresh_button])

"""## Step 1: Entity Suggestion"""

def stepback_entity_candidates(q, llm_client, model="gpt-4o", k_suggest=20, tone="executive"):
    system = "You are an analyst for a Wealth Management application. Respond ONLY with JSON: {\"entities\": [...]}. No extra text."
    user = f"""Question: {q}

Return ONLY: {{"entities": ["name1", "name2", ...]}}
Entities should be concise names from a wealth/order-management knowledge graph.
Tone: {tone}"""

    resp = llm_client.responses.create(
        model=model,
        input=[{"role": "system", "content": system}, {"role": "user", "content": user}],
        max_output_tokens=400,
        temperature=0.2
    )
    data = safe_parse_json(get_output_text(resp)) or {}
    ents = [e.strip() for e in data.get("entities", []) if isinstance(e, str) and e.strip()]

    out, seen = [], set()
    for e in ents:
        n = _norm(e)
        if n and n not in seen:
            seen.add(n)
            out.append(e)
    return out[:k_suggest]

"""## Step 2: Entity Matching"""

def fuzzy_candidate_search(suggested, name_index, max_aliases_per_suggest=5, fuzzy_threshold=80):
    aliases = list(name_index.keys())
    out_aliases = []

    if _HAS_RAPIDFUZZ:
        for s in suggested:
            s_norm = _norm(s)
            if not s_norm:
                continue
            scored = process.extract(s_norm, aliases, scorer=fuzz.token_set_ratio, limit=min(len(aliases), max_aliases_per_suggest * 3))
            picked = [alias for alias, score, _ in scored if score >= fuzzy_threshold]
            out_aliases.extend(picked[:max_aliases_per_suggest])
    else:
        alias_norm_map = {alias: _norm(alias) for alias in aliases}
        for s in suggested:
            s_norm = _norm(s)
            if not s_norm:
                continue
            tokens = [t for t in re.split(r"[^a-z0-9]+", s_norm) if t]
            def score(alias):
                a = alias_norm_map[alias]
                return sum(t in a or a in t for t in tokens)
            ranked = sorted(aliases, key=score, reverse=True)
            picked = [a for a in ranked if score(a) > 0][:max_aliases_per_suggest]
            out_aliases.extend(picked)

    return _dedup(out_aliases, key=lambda x: x.lower())

def resolve_aliases_deterministic(candidate_aliases, name_index, by_id, k_candidates=25):
    matched = []
    for alias in candidate_aliases:
        ids = name_index.get(alias)
        if ids is None:
            continue
        if isinstance(ids, (list, tuple)):
            for nid in ids:
                nid = str(nid)
                if nid in by_id:
                    matched.append((nid, alias))
        else:
            nid = str(ids)
            if nid in by_id:
                matched.append((nid, alias))
    return _dedup(matched, key=lambda x: x[0])[:k_candidates]

def match_entities_to_graph(suggested, name_index, by_id, max_aliases=5, threshold=80, k_candidates=25):
    candidates = fuzzy_candidate_search(suggested, name_index, max_aliases, threshold)
    return resolve_aliases_deterministic(candidates, name_index, by_id, k_candidates)

"""## Step 3: Graph Expansion"""

def expand_nodes(G, seed_ids, hops=1, edge_type_whitelist=None, max_expanded=60, preset_params=None):
    if preset_params:
        max_expanded = preset_params.get("max_expanded", max_expanded)

    def _ok_edge(edata):
        if not edge_type_whitelist:
            return True
        et = edata.get("type") or edata.get("label") or "RELATED"
        return et in edge_type_whitelist

    seen = set(seed_ids)
    q = deque([(nid, 0) for nid in seed_ids])
    edges = []

    while q and len(seen) < max_expanded:
        u, d = q.popleft()
        if u not in G:
            continue

        for v in G[u]:
            for _, edata in G[u][v].items():
                if not _ok_edge(edata):
                    continue
                et = edata.get("type") or edata.get("label") or "RELATED"
                edge = {"source_id": u, "target_id": v, "type": str(et)}
                if "evidence" in edata:
                    edge["evidence"] = edata["evidence"]
                if "source_documents" in edata:
                    edge["source_documents"] = edata["source_documents"]
                edges.append(edge)
            if v not in seen and d < hops:
                seen.add(v)
                q.append((v, d + 1))

        if hasattr(G, "predecessors"):
            for v in G.predecessors(u):
                for _, edata in G[v][u].items():
                    if not _ok_edge(edata):
                        continue
                    et = edata.get("type") or edata.get("label") or "RELATED"
                    edge = {"source_id": v, "target_id": u, "type": str(et)}
                    if "evidence" in edata:
                        edge["evidence"] = edata["evidence"]
                    if "source_documents" in edata:
                        edge["source_documents"] = edata["source_documents"]
                    edges.append(edge)
                if v not in seen and d < hops:
                    seen.add(v)
                    q.append((v, d + 1))

    edges = _dedup(edges, key=lambda e: (e["source_id"], e["target_id"], e["type"]))
    return list(seen), edges

"""## Evidence Collection"""

def collect_edge_evidence(edges, nodes):
    evidence_map = defaultdict(list)
    if not edges:
        return {}
    allowed = set(nodes) if nodes else None

    for e in edges:
        if isinstance(e, dict):
            src = e.get("source_id") or e.get("source")
            dst = e.get("target_id") or e.get("target")
            rel = e.get("type") or "related_to"
            attrs = e
        else:
            continue

        if allowed and (src not in allowed or dst not in allowed):
            continue

        ev = attrs.get("evidence") or attrs.get("snippet") or ""
        prov = {k: attrs.get(k) for k in ("doc_id", "doc_name", "file", "page") if k in attrs}
        key = f"{src}-{rel}-{dst}"
        evidence_map[key].append({"source": src, "target": dst, "type": rel, "evidence": ev, **prov})

    return dict(evidence_map)

def node_context_from_evidence(edge_evidence, by_id):
    edge_evidence = edge_evidence or {}
    by_id = by_id or {}

    def _name(nid):
        meta = by_id.get(nid, {})
        return meta.get("name") or meta.get("label") or str(nid) if isinstance(meta, dict) else str(nid)

    per_node = defaultdict(list)
    for key, ev_list in edge_evidence.items():
        try:
            src, rel, dst = key.split('-', 2)
        except ValueError:
            continue

        src_name, dst_name = _name(src), _name(dst)
        for ev in ev_list or []:
            snippet = ev.get("evidence") or ""
            line = f"{src_name} --({rel})-> {dst_name}"
            if snippet:
                line += f": {snippet}"
            per_node[src].append(line)
            per_node[dst].append(line)

    return {nid: "\n".join(lines) for nid, lines in per_node.items()}

"""## Generate Answer from KG"""

# def grounded_answer_llm(q, nodes, edges, by_id, llm_client, model="gpt-4o", max_tokens=3000, preset_params=None):
#     if preset_params:
#         max_tokens = preset_params.get("max_tokens", max_tokens)
#         model = preset_params.get("model", model)

#     def _fmt_node(n):
#         if isinstance(n, dict):
#             return {"id": n.get("id"), "name": n.get("name"), "type": n.get("type")}
#         meta = by_id.get(n, {})
#         return {"id": n, "name": meta.get("name") or str(n), "type": meta.get("type")}

#     def _fmt_edge(e):
#         if isinstance(e, dict):
#             return {"source": e.get("source_id"), "target": e.get("target_id"), "type": e.get("type")}
#         return {"source": str(e), "target": str(e), "type": "related"}

#     compact_nodes = [_fmt_node(n) for n in (nodes or [])]
#     compact_edges = [_fmt_edge(e) for e in (edges or [])]

#     edge_evidence = collect_edge_evidence(edges or [], nodes or [])
#     node_context = node_context_from_evidence(edge_evidence, by_id)

#     system_msg = "You are a precise wealth-management assistant. Use ONLY the provided graph context. If uncertain, mark it 'unverified'."
#     user_msg = f"""## Question
# {q}

# ## KG Nodes
# {json.dumps(compact_nodes[:30], ensure_ascii=False)}

# ## KG Edges
# {json.dumps(compact_edges[:40], ensure_ascii=False)}

# Use the evidence provided. Do not invent."""

#     try:
#         resp = llm_client.responses.create(
#             model=model,
#             input=[{"role": "system", "content": system_msg}, {"role": "user", "content": user_msg}],
#             max_output_tokens=max_tokens
#         )
#         answer_text = getattr(resp, "output_text", "") or ""
#     except Exception as e:
#         answer_text = f"_Error: {type(e).__name__}_"

#     return {
#         "answer": answer_text,
#         "grounding": {"Question": q, "Nodes": compact_nodes, "Edges": compact_edges, "NodeContext": node_context},
#         "meta": {"model": model, "nodes": len(compact_nodes), "edges": len(compact_edges)}
#     }

def verify_answer_grounding(answer_text, chunks, client, threshold=0.50):
    """
    Verify that answer statements are supported by retrieved chunks
    Returns: dict with confidence score and unsupported claims
    """
    # Extract claims from answer (simple sentence splitting)
    sentences = [s.strip() for s in answer_text.split('.') if len(s.strip()) > 20]

    if not sentences or not chunks:
        return {
            "confidence": 0.0,
            "supported_count": 0,
            "total_claims": len(sentences),
            "unsupported_claims": []
        }

    supported_count = 0
    unsupported = []

    # Sample sentences to avoid excessive API calls
    sample_size = min(15, len(sentences))
    sampled_sentences = sentences[:sample_size]

    for sentence in sampled_sentences:
        # Get embedding for sentence
        sent_embedding = get_question_embedding(client, sentence)

        # Check if any chunk supports this sentence
        max_similarity = 0
        for chunk in chunks[:10]:  # Check top 10 chunks only
            chunk_text = chunk.get("text", "")
            if not chunk_text:
                continue

            chunk_embedding = get_question_embedding(client, chunk_text[:500])
            similarity = cosine_similarity(sent_embedding, chunk_embedding)
            max_similarity = max(max_similarity, similarity)

        if max_similarity >= threshold:
            supported_count += 1
        else:
            unsupported.append(sentence[:100])

    confidence = supported_count / len(sampled_sentences) if sampled_sentences else 0

    return {
        "confidence": confidence,
        "supported_count": supported_count,
        "total_claims": len(sampled_sentences),
        "unsupported_claims": unsupported[:3]  # Return max 3 examples
    }

def grounded_answer_llm(q, nodes, edges, by_id, llm_client, model="gpt-4o", max_tokens=3000, preset_params=None):
    """Generate answer from KG context only with enhanced formatting"""
    if preset_params:
        max_tokens = preset_params.get("max_tokens", max_tokens)
        model = preset_params.get("model", model)
        mode = preset_params.get("_mode", "balanced")
    else:
        mode = "balanced"

    def _fmt_node(n):
        if isinstance(n, dict):
            return {"id": n.get("id"), "name": n.get("name"), "type": n.get("type")}
        meta = by_id.get(n, {})
        return {"id": n, "name": meta.get("name") or str(n), "type": meta.get("type")}

    def _fmt_edge(e):
        if isinstance(e, dict):
            return {"source": e.get("source_id"), "target": e.get("target_id"), "type": e.get("type")}
        return {"source": str(e), "target": str(e), "type": "related"}

    compact_nodes = [_fmt_node(n) for n in (nodes or [])]
    compact_edges = [_fmt_edge(e) for e in (edges or [])]

    edge_evidence = collect_edge_evidence(edges or [], nodes or [])
    node_context = node_context_from_evidence(edge_evidence, by_id)

    # Mode-specific system messages with formatting requirements
    if mode == "concise":
        system_msg = (
            "You are a precise wealth-management assistant. "
            "Use ONLY the provided graph context. .\n\n"
            "FORMATTING REQUIREMENTS:\n"
            "1. Include citations using [KG: entity_name] format\n"
            "2. Use bullet points (•) for any lists\n"
            "3. Add 'Key Point:' before main findings\n"
            "4. Keep sentences under 20 words\n"
            "5. Structure: Brief answer with 2-4 bullets\n"
        )
    elif mode == "deep":
        system_msg = (
            "You are a precise wealth-management assistant. "
            "Use ONLY the provided graph context. \n\n"
            "FORMATTING REQUIREMENTS:\n"
            "1. Include citations using [KG: entity_name] format for key claims\n"
            "2. Use extensive bullet points (•) and sub-bullets (  -) for hierarchical info\n"
            "3. Add multiple markers: 'Key Point:', 'Note:', 'Important:', 'Example:'\n"
            "4. Keep sentences under 25 words\n"
            "5. Structure with clear sections and multiple bullets per section\n"
        )
    else:  # balanced
        system_msg = (
            "You are a precise wealth-management assistant. "
            "Use ONLY the provided graph context.\n\n"
            "CRITICAL FORMATTING REQUIREMENTS:\n"
            "1. ATTRIBUTION: Include citations [1], [2], [3] for key claims\n"
            "2. STRUCTURE: Use clean bullet points (•) for key information\n"
            "3. CLARITY: Maximum 25 words per sentence\n"
            "4. PROFESSIONAL: Write like a business document for analysts\n"
            "5. MARKERS: Use 'Important:', 'Note:', 'Example:' sparingly and professionally\n\n"
            "ANSWER TEMPLATE:\n"
            "**Overview:** [3-5 sentence summary] [1]\n\n"
            "**Key Information:**\n"
            "• [detailed explanation] [2]\n"
            "• [detailed explanation] [3]\n"
            "• [detailed explanation] [1]\n\n"
            "**Important:** [Any important clarification]\n\n"
        )

    # Mode-specific user message templates
    if mode == "concise":
        user_msg = f"""## Question
{q}

## KG Nodes
{json.dumps(compact_nodes[:30], ensure_ascii=False)}

## KG Edges
{json.dumps(compact_edges[:40], ensure_ascii=False)}

Structure your answer:
**Answer:** [1-2 sentences with citation]

**Key Information:**
• [brief detail] [KG: entity]
• [brief detail]
• [brief detail]

Use evidence provided. Do not invent."""

    elif mode == "deep":
        user_msg = f"""## Question
{q}

## KG Nodes
{json.dumps(compact_nodes[:30], ensure_ascii=False)}

## KG Edges
{json.dumps(compact_edges[:40], ensure_ascii=False)}

Structure your answer comprehensively:

**Overview:** [Brief definition with citation]

**Main Points:**
• Point 1: [detailed explanation] [KG: entity]
  - Sub-detail A
  - Sub-detail B
• Point 2: [detailed explanation] [KG: entity]
• Point 3: [detailed explanation]

**Key Insights:**
• Insight 1
• Insight 2

**Note:** [Any clarifications]

Use ALL evidence provided. Be thorough. Do not invent."""

    else:  # balanced
        user_msg = f"""## Question
{q}

## KG Nodes
{json.dumps(compact_nodes[:30], ensure_ascii=False)}

## KG Edges
{json.dumps(compact_edges[:40], ensure_ascii=False)}

Structure your answer:
**Answer:** [Brief overview with citation]

**Key Points:**
• Point 1: [explanation] [KG: entity]
• Point 2: [explanation]
• Point 3: [explanation] [KG: entity]

**Important:** [Any key notes]

Use evidence provided. Do not invent."""

    try:
        resp = llm_client.responses.create(
            model=model,
            input=[{"role": "system", "content": system_msg}, {"role": "user", "content": user_msg}],
            max_output_tokens=max_tokens
        )
        answer_text = getattr(resp, "output_text", "") or ""
    except Exception as e:
        answer_text = f"_Error: {type(e).__name__}_"

    # Apply post-processing
    # answer_text = enhance_answer_formatting(answer_text, has_citations=False, mode=mode)

    return {
        "answer": answer_text,
        "grounding": {"Question": q, "Nodes": compact_nodes, "Edges": compact_edges, "NodeContext": node_context},
        "meta": {"model": model, "nodes": len(compact_nodes), "edges": len(compact_edges), "mode": mode}
    }

"""## Complete KG Pipeline"""

def answer_with_kg(q, G, by_id, name_index, llm_client, hops=1, k_suggest=20, k_candidates=25, preset_params=None):
    if preset_params:
        hops = preset_params.get("hops", hops)
        k_suggest = preset_params.get("k_suggest", k_suggest)
        k_candidates = preset_params.get("k_candidates", k_candidates)

    suggested = stepback_entity_candidates(q, llm_client, k_suggest=k_suggest)
    matched = match_entities_to_graph(suggested, name_index, by_id, k_candidates=k_candidates)
    seed_ids = [nid for nid, _ in matched]

    if not seed_ids:
        return {
            "answer": "_No entities matched in the knowledge graph._",
            "resolved_entities": [],
            "supporting_edges": [],
            "grounding": {},
            "meta": {"reason": "No seeds", "suggested": suggested}
        }

    expanded_nodes, expanded_edges = expand_nodes(G, seed_ids, hops=hops, max_expanded=60, preset_params=preset_params)
    data = grounded_answer_llm(q, expanded_nodes, expanded_edges, by_id, llm_client, preset_params=preset_params)

    data["meta"].update({
        "suggested_entities": suggested,
        "seed_ids": seed_ids,
        "expanded_nodes": len(expanded_nodes),
        "expanded_edges": len(expanded_edges)
    })
    data["resolved_entities"] = expanded_nodes
    data["supporting_edges"] = expanded_edges
    return data

"""## Vector Store Retrieval"""

# Global caches with size limits and TTL
_Q_CACHE = LRUCache()  # Uses settings defaults
_HITS_CACHE = LRUCache(max_size=500, ttl=1800)  # 30 minutes TTL for hits

def programmatic_search(client, vs_id, q, k=3):
    res = client.vector_stores.search(vector_store_id=vs_id, query=q)
    items = []
    for r in getattr(res, "data", []) or []:
        text = ""
        if getattr(r, "content", None) and len(r.content):
            text = getattr(r.content[0], "text", "")
        items.append({
            "filename": getattr(r, "filename", None),
            "file_id": getattr(r, "file_id", None),
            "score": getattr(r, "score", None),
            "text": text[:2500],
            "query": q
        })
    return items[:k]

def retrieve_parallel(client, vs_id, queries, k_each=3, max_workers=6):
    uniq = [q for q in queries if q and not _Q_CACHE.get(q)]
    for q in uniq:
        _Q_CACHE.set(q, True)

    def _fetch(q):
        cached_hits = _HITS_CACHE.get(q)
        if cached_hits is not None:
            return cached_hits
        try:
            hits = programmatic_search(client, vs_id, q, k=k_each)
        except Exception as e:
            import logging
            log = logging.getLogger(__name__)
            log.warning(f"Vector search failed for query '{q[:50]}...': {e}")
            hits = []
        _HITS_CACHE.set(q, hits)
        return hits

    pool = []
    with ThreadPoolExecutor(max_workers=max_workers) as ex:
        futures = {ex.submit(_fetch, q): q for q in uniq}
        for f in as_completed(futures):
            pool.extend(f.result() or [])
    return pool

def text_jaccard(a, b):
    A = set((a or "").lower().split())
    B = set((b or "").lower().split())
    if not A or not B:
        return 0.0
    return len(A & B) / len(A | B)

def mmr_merge(pool, k_final=10, lambda_div=0.4):
    seen = set()
    dedup = []
    for it in pool:
        key = (it.get("filename"), (it.get("text") or "")[:200])
        if key not in seen:
            seen.add(key)
            dedup.append(it)

    scores = [it.get("score") or 0.0 for it in dedup]
    if scores:
        lo, hi = min(scores), max(scores)
        for it in dedup:
            s = it.get("score") or 0.0
            it["norm_score"] = 0.0 if hi == lo else (s - lo) / (hi - lo)
    else:
        for it in dedup:
            it["norm_score"] = 0.0

    selected = []
    while dedup and len(selected) < k_final:
        best, best_val = None, -1e9
        for cand in dedup:
            relevance = cand["norm_score"]
            diversity_pen = 0.0
            if selected:
                sim = max(text_jaccard(cand["text"], s["text"]) for s in selected)
                diversity_pen = sim
            val = (1 - lambda_div) * relevance - lambda_div * diversity_pen
            if val > best_val:
                best, best_val = cand, val
        selected.append(best)
        dedup.remove(best)

    return selected

def rerank_chunks_by_relevance(client, question, chunks, top_k=None, min_chunks=6):
    """
    Re-rank retrieved chunks by actual relevance to the main question
    using embeddings and ensure minimum chunks for completeness
    """
    if not chunks:
        return chunks

    # Get question embedding
    q_embedding = get_question_embedding(client, question)

    # Score each chunk
    for chunk in chunks:
        chunk_text = chunk.get("text", "")
        if not chunk_text:
            chunk["relevance_score"] = 0.0
            continue

        # Get chunk embedding
        chunk_embedding = get_question_embedding(client, chunk_text[:1000])

        # Calculate relevance (cosine similarity)
        relevance = cosine_similarity(q_embedding, chunk_embedding)

        # Combine with original score (if exists)
        original_score = chunk.get("norm_score", 0.5)
        chunk["relevance_score"] = 0.7 * relevance + 0.3 * original_score

    # Sort by relevance
    chunks.sort(key=lambda x: x.get("relevance_score", 0), reverse=True)

    # Adaptive threshold based on score distribution
    scores = [c.get("relevance_score", 0) for c in chunks]

    if scores:
        # Use 15th percentile for very permissive threshold
        percentile_15 = sorted(scores)[max(1, len(scores)//7)]
        RELEVANCE_THRESHOLD = max(0.32, percentile_15 - 0.08)
    else:
        RELEVANCE_THRESHOLD = 0.32

    filtered_chunks = [c for c in chunks if c.get("relevance_score", 0) >= RELEVANCE_THRESHOLD]

    # CRITICAL: Ensure minimum chunks for completeness
    if len(filtered_chunks) < min_chunks:
        # Use MORE chunks when available
        target_count = max(min_chunks, int(len(chunks) * 0.6))  # Use 60% of available chunks
        filtered_chunks = chunks[:target_count]
        print(f"  ⚠️ Only {len([c for c in filtered_chunks if c.get('relevance_score', 0) >= RELEVANCE_THRESHOLD])} chunks above threshold. Using top {len(filtered_chunks)} chunks.")

    return filtered_chunks[:top_k] if top_k else filtered_chunks

def expand_chunk_context(chunks):
    """
    Ensure chunks have complete context and aren't truncated mid-sentence
    """
    expanded = []

    for chunk in chunks:
        text = chunk.get('text', '')

        # If text seems truncated (doesn't end with sentence punctuation)
        if text and len(text) > 100:
            # Check if ends mid-sentence
            last_chars = text.rstrip()[-5:]
            if not any(text.rstrip().endswith(p) for p in ['.', '!', '?', ':', ';', '"', ')', ']']):
                # Add ellipsis to indicate continuation
                text = text.rstrip() + '...'

        chunk['text'] = text
        expanded.append(chunk)

    return expanded

"""## Hybrid Pipeline Helpers"""

# # def build_grounded_prompt(question, compact_nodes, compact_edges, node_context, chunks, instruction=None, preset_params=None):
# #     # Determine answer depth from preset
# #     if preset_params:
# #         mode = preset_params.get("_mode", "balanced")
# #     else:
# #         mode = "balanced"

# #     # Mode-specific instructions
# #     if mode == "concise":
# #         default_instruction = (
# #             "Provide a focused answer using ONLY the KG and passages below. "
# #             "Cite as [1], [2]. Be direct and specific. "
# #             "Every factual statement MUST have a citation."
# #         )
# #     elif mode == "deep":
# #         default_instruction = (
# #             "Provide a COMPREHENSIVE and DETAILED answer using ALL information available in the KG context and passages below.\n\n"

# #             "EXAMPLE OF EXCELLENT ANSWER STRUCTURE:\n"
# #             "Question: What is the CRR and PRR logic for order placement?\n\n"

# #             "Answer:\n"
# #             "1. Overview and Definition\n"
# #             "   The CRR (Customer Risk Rating) and PRR (Product Risk Rating) logic is a validation mechanism "
# #             "that compares the customer's risk profile against the product's risk profile during order placement [1]. "
# #             "This ensures suitable investment recommendations.\n\n"

# #             "2. Detailed Logic and Rules\n"
# #             "   The system applies the following decision rules:\n"
# #             "   - If PRR ≤ CRR: Order proceeds without warning [2]\n"
# #             "   - If PRR = CRR + 1: Warning displayed, requires confirmation [3]\n"
# #             "   - If PRR = CRR + 2: Different behavior by channel [4]:\n"
# #             "     * RM Portal: Warning with approval workflow [5]\n"
# #             "     * Client Portal: Hard stop, order cannot proceed [6]\n"
# #             "   - If PRR > CRR + 2: Hard stop in all channels [7]\n\n"

# #             "3. Channel-Specific Workflows\n"
# #             "   [Detailed breakdown by channel with citations]\n\n"

# #             "4. Approval Process\n"
# #             "   [Step-by-step approval flow with citations]\n\n"

# #             "5. Exceptions and Edge Cases\n"
# #             "   [Any variations or special scenarios]\n\n"

# #             "YOUR TASK - Follow this pattern:\n"
# #             "✓ Start with clear overview/definition\n"
# #             "✓ Break down into logical numbered sections (5-8 sections for complex topics)\n"
# #             "✓ Within each section, use sub-bullets or numbered sub-points\n"
# #             "✓ Cite EVERY factual statement with [1], [2], [3], etc.\n"
# #             "✓ Include ALL details, rules, conditions, and exceptions from passages\n"
# #             "✓ When sources provide step-by-step processes, reproduce them completely\n"
# #             "✓ When sources mention variations (e.g., 'Implementation A vs B'), cover both\n"
# #             "✓ Use confident, authoritative language - avoid hedging when sources are clear\n"
# #             "✓ Synthesize information across multiple passages for complete coverage\n"
# #             "✓ Aim for thoroughness: 1500-3000 words when sources are detailed\n\n"

# #             "WHAT TO AVOID:\n"
# #             "✗ Do NOT add information not present in sources\n"
# #             "✗ Do NOT use phrases like 'possibly', 'might', 'may be' unless source uses them\n"
# #             "✗ Do NOT omit important details that are in the sources\n"
# #             "✗ Do NOT create vague summaries - be specific with rules, numbers, conditions\n\n"

# #             "If sources are comprehensive → Your answer must be comprehensive\n"
# #             "If sources have gaps → Note gaps briefly at the very end only"
# #         )
# #     else:  # balanced
# #         default_instruction = (
# #             "Provide a clear answer using ONLY the KG and passages below. "
# #             "Include main points with explanations. Cite as [1], [2]. "
# #             "State information confidently when supported by sources. "
# #             "Every factual statement MUST have a citation."
# #         )

# #     instructions = instruction or default_instruction

# #     lines = ["You are a precise wealth-management assistant.", instructions, "", "## Question", question, ""]

# #     if compact_nodes:
# #         lines.append("## KG Nodes")
# #         for n in compact_nodes[:50]:
# #             lines.append(f"- {n.get('name')} ({n.get('type')}) [id={n.get('id')}]")
# #         lines.append("")

# #     if compact_edges:
# #         lines.append("## KG Edges")
# #         for e in compact_edges[:80]:
# #             lines.append(f"- {e.get('source')} --({e.get('type')})-> {e.get('target')}")
# #         lines.append("")

# #     if node_context:
# #         lines.append("## KG Context")
# #         for nid, ctx in list(node_context.items())[:30]:
# #             lines.append(f"- {nid}:")
# #             lines.append(indent(ctx[:1500]))
# #         lines.append("")

# #     lines.append("## Retrieved Passages")
# #     for i, c in enumerate(chunks, 1):
# #         src = c.get("filename") or c.get("file_id")
# #         txt = (c.get("text") or "")[:1800]
# #         lines.append(f"[{i}] ({src})")
# #         lines.append(indent(txt))
# #         lines.append("")

# #     return "\n".join(lines)

# def build_grounded_prompt(question, compact_nodes, compact_edges, node_context, chunks, instruction=None, preset_params=None):
#     """Build comprehensive prompt with KG + vector context and enhanced formatting instructions"""

#     # Determine answer depth from preset
#     if preset_params:
#         mode = preset_params.get("_mode", "balanced")
#     else:
#         mode = "balanced"

#     # Mode-specific instructions with DETAILED formatting requirements
#     if mode == "concise":
#         default_instruction = (
#             "Provide a FOCUSED answer using ONLY the KG and passages below.\n\n"

#             "CRITICAL FORMATTING REQUIREMENTS:\n"
#             "1. ATTRIBUTION: Include 1-2 citations [1], [2] for key claims\n"
#             "2. STRUCTURE: Use 3-5 bullet points (•) for key information\n"
#             "3. MARKERS: Add 'Key Point:' and 'Note:' markers\n"
#             "4. CLARITY: Maximum 20 words per sentence\n"
#             "5. LENGTH: Keep total answer under 250 words\n\n"

#             "ANSWER TEMPLATE:\n"
#             "**Direct Answer:** [1-2 sentence summary] [1]\n\n"
#             "**Key Points:**\n"
#             "• Point 1: [brief detail] [2]\n"
#             "• Point 2: [brief detail]\n"
#             "• Point 3: [brief detail] [1]\n\n"
#             "**Note:** [Any important clarification]\n\n"

#             "Every factual statement MUST have a citation."
#         )

#     elif mode == "deep":
#         default_instruction = (
#             "Provide a COMPREHENSIVE and DETAILED answer using ALL information in the KG context and passages below.\n\n"

#             "CRITICAL FORMATTING REQUIREMENTS:\n"
#             "1. ATTRIBUTION:\n"
#             "   - Include 3-5 citations minimum using [1], [2], [3], [4], [5]\n"
#             "   - Also cite KG entities as [KG: entity_name] when using graph information\n"
#             "   - Cite EVERY factual statement, rule, number, or condition\n"
#             "2. STRUCTURE:\n"
#             "   - Use extensive bullet points (•) for all lists and findings\n"
#             "   - Use sub-bullets (  -) for hierarchical details\n"
#             "   - Create 5-8 numbered sections for complex topics\n"
#             "   - Each section should have 3-6 bullet points\n"
#             "3. MARKERS:\n"
#             "   - Use multiple markers: 'Key Point:', 'Note:', 'Important:', 'Example:', 'Analysis:', 'Evidence:'\n"
#             "   - Add at least 5-8 markers throughout the answer\n"
#             "4. CLARITY:\n"
#             "   - Maximum 25 words per sentence\n"
#             "   - One concept per sentence\n"
#             "   - Short paragraphs (2-3 sentences max between bullets)\n"
#             "5. COMPLETENESS:\n"
#             "   - Answer should be 1000-2500 words for complex topics\n"
#             "   - Cover ALL details from sources\n"
#             "   - Include ALL rules, conditions, and exceptions mentioned\n\n"

#             "EXAMPLE STRUCTURE FOR EXCELLENT ANSWERS:\n"
#             "**Overview:**\n"
#             "[2-3 sentence definition/introduction] [1]\n\n"

#             "**1. Main Topic Area**\n"
#             "Key Point: [Introduction to this section] [2]\n"
#             "• Finding 1: [detailed explanation with specifics] [3]\n"
#             "  - Sub-detail A\n"
#             "  - Sub-detail B\n"
#             "• Finding 2: [detailed explanation] [KG: entity] [4]\n"
#             "• Finding 3: [detailed explanation] [5]\n\n"

#             "**2. Second Topic Area**\n"
#             "Important: [Key insight for this section]\n"
#             "• Point A: [comprehensive detail] [1]\n"
#             "• Point B: [comprehensive detail] [6]\n"
#             "• Point C: [comprehensive detail] [KG: entity]\n\n"

#             "**3. Detailed Rules/Process**\n"
#             "Analysis: [Context for rules below]\n"
#             "• Rule 1: [complete specification] [7]\n"
#             "  - Condition A\n"
#             "  - Condition B\n"
#             "• Rule 2: [complete specification] [2]\n"
#             "• Rule 3: [complete specification] [8]\n\n"

#             "**4. Variations/Edge Cases**\n"
#             "Note: [Introduction to variations]\n"
#             "• Variation 1: [detailed explanation] [9]\n"
#             "• Variation 2: [detailed explanation] [3]\n\n"

#             "**5. Additional Considerations**\n"
#             "• Consideration 1 [10]\n"
#             "• Consideration 2 [KG: entity]\n\n"

#             "**Key Insights:**\n"
#             "• Critical takeaway 1\n"
#             "• Critical takeaway 2\n\n"

#             "YOUR TASK:\n"
#             "✓ Follow this pattern: Clear sections with extensive bullets\n"
#             "✓ Cite EVERY factual claim\n"
#             "✓ Include ALL details, don't summarize if sources are detailed\n"
#             "✓ Use confident language when sources are clear\n"
#             "✓ Add 5-8 markers throughout\n"
#             "✓ Aim for completeness: if sources have 10 points, include all 10\n\n"

#             "AVOID:\n"
#             "✗ Vague summaries when sources are specific\n"
#             "✗ Omitting details present in sources\n"
#             "✗ Hedging language ('might', 'possibly') unless source uses it\n"
#         )

#     else:  # balanced
#         default_instruction = (
#             "Provide a CLEAR and WELL-STRUCTURED answer using ONLY the KG and passages below.\n\n"

#             "CRITICAL FORMATTING REQUIREMENTS:\n"
#             "1. ATTRIBUTION:\n"
#             "   - Include 2-3 citations using [1], [2], [3] format\n"
#             "   - Also cite KG entities as [KG: entity_name]\n"
#             "   - Every key fact needs a citation\n"
#             "2. STRUCTURE:\n"
#             "   - Use bullet points (•) extensively for lists and findings\n"
#             "   - Use sub-bullets (  -) where appropriate\n"
#             "   - Create 3-5 clear sections or logical groups\n"
#             "3. MARKERS:\n"
#             "   - Add 3-5 markers: 'Key Point:', 'Note:', 'Important:', 'Example:'\n"
#             "   - Use markers to highlight critical information\n"
#             "4. CLARITY:\n"
#             "   - Maximum 25 words per sentence\n"
#             "   - One main idea per sentence\n"
#             "   - Short paragraphs between bullet sections\n"
#             "5. LENGTH: 400-800 words typically\n\n"

#             "ANSWER TEMPLATE:\n"
#             "**Overview:** [2-3 sentence introduction] [1]\n\n"

#             "**Main Findings:**\n"
#             "Key Point: [Context for this section]\n"
#             "• Finding 1: [explanation with details] [2]\n"
#             "• Finding 2: [explanation with details] [KG: entity]\n"
#             "• Finding 3: [explanation with details] [3]\n\n"

#             "**Additional Details:**\n"
#             "• Detail 1: [information] [1]\n"
#             "• Detail 2: [information] [4]\n"
#             "  - Sub-point A\n"
#             "  - Sub-point B\n\n"

#             "**Important:** [Key notes or considerations]\n\n"

#             "Every factual statement MUST have a citation."
#         )

#     instructions = instruction or default_instruction

#     # Build prompt sections
#     lines = [
#         "You are a precise wealth-management assistant.\n",
#         "## Instructions\n",
#         instructions + "\n",
#         "",
#         "## FORMATTING REMINDER\n",
#         "Before you write, remember:\n",
#         "✓ Use [1], [2], [3] for passage citations\n",
#         "✓ Use [KG: entity_name] for knowledge graph references\n",
#         "✓ Use bullets (•) extensively throughout\n",
#         "✓ Add markers (Key Point:, Note:, Important:, etc.)\n",
#         "✓ Keep sentences under 25 words\n",
#         "✓ Cite every factual statement\n",
#         "\n",
#         "## Question\n",
#         question + "\n",
#         ""
#     ]

#     # Add KG context
#     if compact_nodes:
#         lines.append("## KG Nodes")
#         for n in compact_nodes[:50]:
#             lines.append(f"- {n.get('name')} ({n.get('type')}) [id={n.get('id')}]")
#         lines.append("")

#     if compact_edges:
#         lines.append("## KG Edges")
#         for e in compact_edges[:80]:
#             lines.append(f"- {e.get('source')} --({e.get('type')})-> {e.get('target')}")
#         lines.append("")

#     if node_context:
#         lines.append("## KG Context")
#         for nid, ctx in list(node_context.items())[:30]:
#             lines.append(f"- {nid}:")
#             lines.append(indent(ctx[:1500]))
#         lines.append("")

#     # Add retrieved passages
#     lines.append("## Retrieved Passages")
#     for i, c in enumerate(chunks, 1):
#         src = c.get("filename") or c.get("file_id")
#         txt = (c.get("text") or "")[:1800]
#         lines.append(f"[{i}] ({src})")
#         lines.append(indent(txt))
#         lines.append("")

#     return "\n".join(lines)

# def enhance_answer_formatting(answer_text, has_citations=True, mode="balanced"):
#     """
#     Post-process LLM output to ensure compliance with formatting requirements.
#     Adds missing elements if not present.

#     Args:
#         answer_text: The raw answer from LLM
#         has_citations: Whether passages were provided (affects citation addition)
#         mode: Answer mode (concise/balanced/deep) - affects expectations

#     Returns:
#         Enhanced answer text with better formatting
#     """
#     import re

#     if not answer_text or answer_text.startswith("_Error"):
#         return answer_text

#     # Define expected minimums based on mode
#     if mode == "concise":
#         min_markers = 1
#         min_bullets = 3
#         min_citations = 1
#     elif mode == "deep":
#         min_markers = 5
#         min_bullets = 8
#         min_citations = 3
#     else:  # balanced
#         min_markers = 3
#         min_bullets = 5
#         min_citations = 2

#     # Count existing formatting elements
#     markers = ['Key Point:', 'Note:', 'Important:', 'Example:', 'Key Insight:',
#                'Analysis:', 'Evidence:', 'Overview:', 'Summary:']
#     marker_count = sum(1 for marker in markers if marker in answer_text)

#     bullet_count = answer_text.count('•') + answer_text.count('\n- ') + answer_text.count('\n* ')

#     citation_count = len(re.findall(r'\[(?:\d+|KG:[^\]]+)\]', answer_text))

#     # Enhancement 1: Add markers if missing
#     if marker_count < min_markers:
#         paragraphs = answer_text.split('\n\n')

#         # Add "Key Point:" to first substantial paragraph if not present
#         if len(paragraphs) > 0 and not any(marker in paragraphs[0] for marker in markers):
#             if len(paragraphs[0]) > 50 and not paragraphs[0].startswith('#'):
#                 paragraphs[0] = "**Key Point:** " + paragraphs[0]
#                 marker_count += 1

#         # Add "Note:" or "Important:" to last paragraph if still need more markers
#         if len(paragraphs) > 1 and marker_count < min_markers:
#             last_para = paragraphs[-1]
#             if len(last_para) > 30 and not any(marker in last_para for marker in markers):
#                 if not last_para.startswith('#') and not last_para.startswith('**'):
#                     paragraphs[-1] = "**Note:** " + last_para
#                     marker_count += 1

#         # Add "Important:" to middle section if still need markers
#         if len(paragraphs) > 2 and marker_count < min_markers:
#             mid_idx = len(paragraphs) // 2
#             if not any(marker in paragraphs[mid_idx] for marker in markers):
#                 if len(paragraphs[mid_idx]) > 40:
#                     paragraphs[mid_idx] = "**Important:** " + paragraphs[mid_idx]

#         answer_text = '\n\n'.join(paragraphs)

#     # Enhancement 2: Add citations if missing and passages were provided
#     if has_citations and citation_count < min_citations:
#         sentences = re.split(r'(?<=[.!?])\s+', answer_text)
#         added_citations = 0

#         for i, sent in enumerate(sentences):
#             # Skip if already has citation, is too short, or is a header
#             if '[' in sent or len(sent) < 40 or sent.startswith('#') or sent.startswith('**'):
#                 continue

#             # Add citation to sentences that look like factual claims
#             if any(keyword in sent.lower() for keyword in ['is', 'are', 'must', 'should', 'will', 'can', 'include', 'require']):
#                 # Add citation before the period
#                 sentences[i] = re.sub(r'([.!?])$', r' [1]\1', sent)
#                 added_citations += 1

#                 if citation_count + added_citations >= min_citations:
#                     break

#         if added_citations > 0:
#             answer_text = ' '.join(sentences)

#     # Enhancement 3: Convert lists to bullets if needed
#     if bullet_count < min_bullets:
#         # Look for numbered lists and convert some to bullets
#         lines = answer_text.split('\n')
#         for i, line in enumerate(lines):
#             # Convert "1. " or "2. " etc to bullets
#             if re.match(r'^\s*\d+\.\s+', line):
#                 lines[i] = re.sub(r'^(\s*)\d+\.\s+', r'\1• ', line)
#                 bullet_count += 1

#                 if bullet_count >= min_bullets:
#                     break

#         answer_text = '\n'.join(lines)

#     # Enhancement 4: Ensure proper section headers
#     # Add bold to headers that look like sections but aren't formatted
#     answer_text = re.sub(
#         r'^([A-Z][^.!?:\n]{3,50}):(?!\*)',  # Find "Title:" not followed by *
#         r'**\1:**',  # Make it **Title:**
#         answer_text,
#         flags=re.MULTILINE
#     )

#     return answer_text


def build_grounded_messages(question, compact_nodes, compact_edges, node_context, chunks, mode=None, preset_params=None):
    """
    Build system and user messages for hybrid answer generation.
    Returns: (system_msg, user_msg) tuple

    This replaces the old build_grounded_prompt that returned a single string.
    """

    # Determine mode
    if mode is None and preset_params:
        mode = preset_params.get("_mode", "balanced")
    elif mode is None:
        mode = "balanced"

    # ============= SYSTEM MESSAGE (Instructions) =============

    if mode == "concise":
        system_msg = (
            "You are a precise wealth-management assistant. "
            "Answer using ONLY the provided KG context and passages.\n\n"

            "CRITICAL FORMATTING REQUIREMENTS:\n"
            "1. ATTRIBUTION: Include citations [1], [2] for key claims\n"
            "2. STRUCTURE: Use 3-5 bullet points (•) for key information\n"
            "3. MARKERS: Use 'Important:' sparingly and professionally\n"
            "4. CLARITY: Maximum 20 words per sentence\n"
            "5. LENGTH: Keep total answer under 250 words\n\n"

            "ANSWER TEMPLATE:\n"
            "**Answer:** [2-5 sentence summary] [1]\n\n"
            "**Key Information:**\n"
            "• [brief detail] [2]\n"
            "• [brief detail]\n"
            "• [brief detail] [1]\n\n"
            "**Important:** [Any important clarification]\n\n"

            "Every factual statement should have a citation. "
        )

    elif mode == "deep":
        system_msg = (
            "You are a precise wealth-management assistant. "
            "Answer using ONLY the provided KG context and passages.\n\n"

            "CRITICAL FORMATTING REQUIREMENTS:\n"
            "1. ATTRIBUTION:\n"
            "   - Include citations using [1], [2], [3], [4], [5]\n"
            "   - Also cite KG entities as [KG: entity_name]\n"
            "   - Cite EVERY factual statement, rule, number, or condition\n"
            "2. STRUCTURE:\n"
            "   - Use extensive bullet points (•) for all lists and findings\n"
            "   - Use sub-bullets (  -) for hierarchical details\n"
            "   - Create 5-8 numbered sections for complex topics\n"
            "   - Each section should have 3-6 bullet points\n"
            "3. MARKERS:\n"
            "   - Use professional markers: 'Important:', 'Note:', 'Example:', 'Analysis:', 'Evidence:'\n"
            "   - Add 3-5 markers throughout the answer for clarity\n"
            "4. CLARITY:\n"
            "   - Maximum 25 words per sentence\n"
            "   - One concept per sentence\n"
            "   - Short paragraphs (2-3 sentences max between bullets)\n"
            "5. COMPLETENESS:\n"
            "   - Answer should be 1000-2500 words for complex topics\n"
            "   - Cover ALL details from sources\n"
            "   - Include ALL rules, conditions, and exceptions mentioned\n\n"

            "ANSWER STRUCTURE:\n"
            "**Overview:** [3-5 sentence definition] [1]\n\n"
            "**1. Main Topic**\n"
            "Key Point: [Introduction] [2]\n"
            "• Finding 1: [detailed explanation] [3]\n"
            "  - Sub-detail A\n"
            "  - Sub-detail B\n"
            "• Finding 2: [detailed explanation] [KG: entity] [4]\n\n"
            "**2. Second Topic**\n"
            "Important: [Key insight]\n"
            "• Point A: [comprehensive detail] [1]\n"
            "• Point B: [comprehensive detail] [5]\n\n"
            "[Continue with 3-6 more sections following this pattern]\n\n"
            "**Key Insights:**\n"
            "• Critical takeaway 1\n"
            "• Critical takeaway 2\n\n"

            "Follow this pattern. Cite EVERY factual claim. "
            "Include ALL details from sources. "
        )

    else:  # balanced
        system_msg = (
            "You are a precise wealth-management assistant. "
            "Answer using ONLY the provided KG context and passages.\n\n"

            "CRITICAL FORMATTING REQUIREMENTS:\n"
            "1. ATTRIBUTION:\n"
            "   - Include citations using [1], [2], [3] format\n"
            "   - Also cite KG entities as [KG: entity_name]\n"
            "   - Every key fact needs a citation\n"
            "2. STRUCTURE:\n"
            "   - Use bullet points (•) extensively for lists and findings\n"
            "   - Use sub-bullets (  -) where appropriate\n"
            "   - Create 3-5 clear sections or logical groups\n"
            "3. MARKERS:\n"
            "   - Add 3-5 markers: 'Key Point:', 'Note:', 'Important:', 'Example:'\n"
            "   - Use markers to highlight critical information\n"
            "4. CLARITY:\n"
            "   - Maximum 25 words per sentence\n"
            "   - One main idea per sentence\n"
            "   - Short paragraphs between bullet sections\n"
            "5. LENGTH: 400-800 words typically\n\n"

            "ANSWER TEMPLATE:\n"
            "**Overview:** [2-3 sentence introduction] [1]\n\n"

            "**Key Information:**\n"
            "• [explanation with details] [2]\n"
            "• [explanation with details] [KG: entity]\n"
            "• [explanation with details] [3]\n\n"

            "**Important Details:**\n"
            "• [information] [1]\n"
            "• [information] [4]\n"
            "  - Sub-point A\n"
            "  - Sub-point B\n\n"

            "**Important:** [Key notes or considerations]\n\n"

            "Every factual statement MUST have a citation. "
        )

    # ============= USER MESSAGE (Context + Question) =============

    user_parts = []

    # Add formatting reminder
    user_parts.append("## FORMATTING REMINDER")
    user_parts.append("Before you write, remember:")
    user_parts.append("✓ Use [1], [2], [3] for passage citations")
    user_parts.append("✓ Use [KG: entity_name] for knowledge graph references")
    user_parts.append("✓ Use bullets (•) extensively throughout")
    user_parts.append("✓ Add markers (Key Point:, Note:, Important:, etc.)")
    user_parts.append("✓ Keep sentences under 25 words")
    user_parts.append("✓ Cite every factual statement\n")

    # Add question
    user_parts.append("## Question")
    user_parts.append(question)
    user_parts.append("")

    # Add KG context
    if compact_nodes:
        user_parts.append("## KG Nodes")
        for n in compact_nodes[:50]:
            user_parts.append(f"- {n.get('name')} ({n.get('type')}) [id={n.get('id')}]")
        user_parts.append("")

    if compact_edges:
        user_parts.append("## KG Edges")
        for e in compact_edges[:80]:
            user_parts.append(f"- {e.get('source')} --({e.get('type')})-> {e.get('target')}")
        user_parts.append("")

    if node_context:
        user_parts.append("## KG Context")
        for nid, ctx in list(node_context.items())[:30]:
            user_parts.append(f"- {nid}:")
            user_parts.append(indent(ctx[:1500]))
        user_parts.append("")

    # Add retrieved passages
    user_parts.append("## Retrieved Passages")
    for i, c in enumerate(chunks, 1):
        src = c.get("filename") or c.get("file_id")
        txt = (c.get("text") or "")[:1800]
        user_parts.append(f"[{i}] ({src})")
        user_parts.append(indent(txt))
        user_parts.append("")

    user_msg = "\n".join(user_parts)

    return system_msg, user_msg

def kg_anchors(resolved_entities, supporting_edges, by_id):
    """Extract entity names and provenance from KG results"""
    ent_names = []
    for n in resolved_entities or []:
        if isinstance(n, dict):
            ent_names.append(n.get("name") or n.get("label") or n.get("id"))
        else:
            meta = (by_id or {}).get(n, {}) if isinstance(by_id, dict) else {}
            ent_names.append(meta.get("name") or meta.get("label") or str(n))
    ent_names = [x for x in ent_names if x]
    ent_names = list(dict.fromkeys(ent_names))  # Deduplicate

    provenance = []
    for e in supporting_edges or []:
        attrs = None
        if isinstance(e, dict):
            attrs = e
        elif isinstance(e, (list, tuple)) and len(e) >= 3 and isinstance(e[2], dict):
            attrs = e[2]
        if attrs:
            prov = {
                k: attrs.get(k)
                for k in ("doc_id", "doc_name", "file", "page", "source", "module", "market")
                if attrs.get(k) is not None
            }
            if prov:
                provenance.append(prov)
    return ent_names, provenance

def expand_queries_from_kg(question, entity_names, supporting_edges, k_max=6):
    """Create focused retrieval queries from KG context"""
    queries = []

    # Always start with exact question
    if question:
        queries.append(question)

    # Extract key terms from the question itself
    question_lower = question.lower()

    # Question-specific keyword extraction
    key_phrases = []

    if "crr" in question_lower or "prr" in question_lower:
        key_phrases = [
            "customer risk rating product risk rating",
            "CRR PRR validation rules",
            "risk profile comparison order placement"
        ]
    elif "otp" in question_lower:
        key_phrases = [
            "OTP mobile number registration",
            "two factor authentication folio",
            "circular SEBI mobile OTP"
        ]
    elif "transaction slip" in question_lower:
        key_phrases = [
            "transaction slip upload mandatory",
            "physical mode document upload",
            "slip requirement transaction type"
        ]
    elif "inflow" in question_lower and "outflow" in question_lower:
        key_phrases = [
            "inflow outflow simultaneous placement",
            "purchase redemption together submit",
            "separate transaction clubbing restriction"
        ]
    elif "signature" in question_lower and "nomination" in question_lower:
        key_phrases = [
            "signature requirement joint account",
            "nomination form signing authority",
            "transaction slip authorization holder"
        ]
    elif "nominee" in question_lower:
        key_phrases = [
            "nominee mandatory fields capture",
            "nomination data requirements",
            "nominee information folio creation"
        ]
    elif "order expiry" in question_lower:
        key_phrases = [
            "order expiry validity period",
            "expiration implementation variation",
            "order timeout cancellation"
        ]
    elif "rm portal" in question_lower or "client portal" in question_lower:
        key_phrases = [
            "RM portal client portal difference",
            "order process flow channel variation",
            "portal specific workflow"
        ]
    elif "order status" in question_lower:
        key_phrases = [
            "order status lifecycle journey",
            "order stages implementation",
            "status transitions workflow"
        ]
    elif "model order" in question_lower:
        key_phrases = [
            "model order portfolio journey",
            "model prerequisites implementation",
            "portfolio based order"
        ]
    elif "folio" in question_lower:
        key_phrases = [
            "folio number investor multiple",
            "single investor multiple folios",
            "folio AMC relationship"
        ]
    else:
        # Generic fallback
        # Extract important nouns from question
        words = question_lower.split()
        key_words = [w for w in words if len(w) > 4 and w not in ['what', 'when', 'where', 'which', 'would', 'should', 'could']]
        if key_words:
            key_phrases = [' '.join(key_words[:4])]

    # Add key phrase queries
    for phrase in key_phrases[:3]:
        if phrase and phrase not in queries:
            queries.append(phrase)

    # IMPORTANT: Don't append entity names that create malformed queries
    # Only add entity if it's clearly relevant and short
    for name in entity_names[:1]:
        if name and len(name.split()) <= 3 and len(queries) < k_max:
            # Only add if entity is actually mentioned in question or is clearly relevant
            if name.lower() in question_lower or any(word in name.lower() for word in ['order', 'folio', 'nominee', 'risk']):
                queries.append(f"{name} details")

    # Deduplicate
    seen = set()
    result = []
    for q in queries:
        q_clean = q.strip().lower()
        if q and q_clean not in seen and len(q) >= 10:
            seen.add(q_clean)
            result.append(q)
        if len(result) >= k_max:
            break

    return result[:k_max]

def build_citation_map(chunks):
    """Build citation indices and source maps"""
    file_to_idxs = {}
    idx_to_src = {}
    for i, c in enumerate(chunks, 1):
        fn = c.get("filename") or c.get("file_id") or f"file_{i}"
        file_to_idxs.setdefault(fn, []).append(i)
        idx_to_src[i] = {"filename": c.get("filename"), "file_id": c.get("file_id")}
    return file_to_idxs, idx_to_src

"""## Complete Hybrid Pipeline"""

# def hybrid_answer(q, kg_result, by_id, client, vs_id, model="gpt-4o", max_chunks=10, k_each=3, lambda_div=0.4, preset_params=None):
#     if preset_params:
#         max_chunks = preset_params.get("max_chunks", max_chunks)
#         k_each = preset_params.get("k_each", k_each)
#         lambda_div = preset_params.get("lambda_div", lambda_div)
#         model = preset_params.get("model", model)

#     ents, prov = kg_anchors(kg_result.get("resolved_entities"), kg_result.get("supporting_edges"), by_id)
#     # subqs = expand_queries_from_kg(q, ents, kg_result.get("supporting_edges"))

#     # print(f"  → Generated {len(subqs)} sub-queries")

#     subqs = expand_queries_from_kg(q, ents, kg_result.get("supporting_edges"))

#     # Add reformulated versions for better coverage
#     reformulations = []

#     # Extract key terms from question
#     q_lower = q.lower()

#     # Add imperative reformulations
#     if q.startswith("What"):
#         reformulations.append(q.replace("What is", "Explain", 1))
#         reformulations.append(q.replace("What are", "List all", 1))
#         reformulations.append(q.replace("What", "Describe", 1))
#     elif q.startswith("Which"):
#         reformulations.append(q.replace("Which", "What are all the", 1))
#     elif q.startswith("Can"):
#         reformulations.append(q.replace("Can I", "Is it possible to", 1))
#         reformulations.append(q.replace("Can", "Rules for", 1))
#     elif q.startswith("Whose"):
#         reformulations.append(q.replace("Whose", "Which person's", 1))
#         reformulations.append(q.replace("Whose", "Who must provide", 1))

#     # Add context-enriched versions
#     reformulations.append(f"{q} requirements rules conditions")
#     reformulations.append(f"{q} process workflow steps")

#     # Combine unique queries
#     all_queries = subqs[:]
#     for ref in reformulations:
#         if ref and ref not in all_queries and len(all_queries) < 10:
#             all_queries.append(ref)

#     subqs = all_queries[:10]  # Cap at 10 queries

#     print(f"  → Generated {len(subqs)} sub-queries")

#     for i, sq in enumerate(subqs, 1):
#         print(f"     {i}. {sq[:80]}...")

#     pool = retrieve_parallel(client, vs_id, subqs, k_each=k_each)
#     print(f"  → Retrieved {len(pool)} raw chunks from vector store")

#     if not pool:
#         print(f"  ⚠️  WARNING: Vector store returned 0 results!")
#         print(f"     Check: Is VECTOR_STORE_ID correct? Are there documents in the store?")

#     # curated = mmr_merge(pool, k_final=max_chunks, lambda_div=lambda_div)
#     # print(f"  → After MMR: {len(curated)} curated chunks")

#     # # Re-rank and reassign to same variable
#     # curated = rerank_chunks_by_relevance(client, q, curated, top_k=max_chunks)
#     # print(f"  → After re-ranking: {len(curated)} high-relevance chunks")

#     curated = mmr_merge(pool, k_final=max_chunks*5, lambda_div=lambda_div)
#     print(f"  → After MMR: {len(curated)} curated chunks")

#     # Add re-ranking with adaptive threshold
#     curated = rerank_chunks_by_relevance(
#         client, q, curated,
#         top_k=max_chunks,
#         min_chunks=max(6, max_chunks//2)  # Ensure minimum chunks
#     )

#     # Expand context to ensure completeness
#     curated = expand_chunk_context(curated)

#     print(f"  → After re-ranking: {len(curated)} high-relevance chunks")

#     # Debug: Show what chunks we're actually using
#     print(f"  → Chunk relevance scores:")
#     for i, chunk in enumerate(curated[:5], 1):
#         score = chunk.get('relevance_score', 0)
#         filename = chunk.get('filename', 'unknown')[:40]
#         text_preview = chunk.get('text', '')[:80].replace('\n', ' ')
#         print(f"     [{i}] Score: {score:.3f} | {filename} | {text_preview}...")

#     node_ctx = kg_result.get("grounding", {}).get("NodeContext", {})
#     compact_nodes = kg_result.get("grounding", {}).get("Nodes", [])
#     compact_edges = kg_result.get("grounding", {}).get("Edges", [])

#     prompt = build_grounded_prompt(q, compact_nodes, compact_edges, node_ctx, curated, preset_params=preset_params)
#     resp = client.responses.create(model=model, input=prompt)
#     answer = resp.output_text


#     # Verify grounding
#     # Track verification for logging only (don't display warnings)
#     verification = verify_answer_grounding(answer, curated, client)
#     # Only log to console, not in answer output
#     if verification['confidence'] < 0.25:  # Only if EXTREMELY low
#         print(f"  → Grounding confidence: {verification['confidence']:.2f}")

#     file_to_idxs, idx_to_src = build_citation_map(curated)


#     return {
#         "answer": answer,
#         "verification": verification,  # Add to output
#         "subqueries": subqs,
#         "curated_chunks": curated,
#         "citation_index_to_source": idx_to_src,
#         "files_to_citation_indices": file_to_idxs,
#         "provenance_from_kg": prov,
#         "model_used": model,
#         "prompt_chars": len(prompt)
#     }


def hybrid_answer(q, kg_result, by_id, client, vs_id, model="gpt-4o", max_chunks=10, k_each=3, lambda_div=0.4, preset_params=None):
    """Complete hybrid pipeline with enhanced formatting"""
    if preset_params:
        max_chunks = preset_params.get("max_chunks", max_chunks)
        k_each = preset_params.get("k_each", k_each)
        lambda_div = preset_params.get("lambda_div", lambda_div)
        model = preset_params.get("model", model)
        mode = preset_params.get("_mode", "balanced")
    else:
        mode = "balanced"

    ents, prov = kg_anchors(kg_result.get("resolved_entities"), kg_result.get("supporting_edges"), by_id)
    subqs = expand_queries_from_kg(q, ents, kg_result.get("supporting_edges"))

    # Add reformulated versions for better coverage
    reformulations = []
    q_lower = q.lower()

    if q.startswith("What"):
        reformulations.append(q.replace("What is", "Explain", 1))
        reformulations.append(q.replace("What are", "List all", 1))
        reformulations.append(q.replace("What", "Describe", 1))
    elif q.startswith("Which"):
        reformulations.append(q.replace("Which", "What are all the", 1))
    elif q.startswith("Can"):
        reformulations.append(q.replace("Can I", "Is it possible to", 1))
        reformulations.append(q.replace("Can", "Rules for", 1))
    elif q.startswith("Whose"):
        reformulations.append(q.replace("Whose", "Which person's", 1))
        reformulations.append(q.replace("Whose", "Who must provide", 1))

    reformulations.append(f"{q} requirements rules conditions")
    reformulations.append(f"{q} process workflow steps")

    all_queries = subqs[:]
    for ref in reformulations:
        if ref and ref not in all_queries and len(all_queries) < 10:
            all_queries.append(ref)

    subqs = all_queries[:10]

    print(f"  → Generated {len(subqs)} sub-queries")
    for i, sq in enumerate(subqs, 1):
        print(f"     {i}. {sq[:80]}...")

    pool = retrieve_parallel(client, vs_id, subqs, k_each=k_each)
    print(f"  → Retrieved {len(pool)} raw chunks from vector store")

    if not pool:
        print(f"  ⚠️  WARNING: Vector store returned 0 results!")

    curated = mmr_merge(pool, k_final=max_chunks*5, lambda_div=lambda_div)
    print(f"  → After MMR: {len(curated)} curated chunks")

    curated = rerank_chunks_by_relevance(
        client, q, curated,
        top_k=max_chunks,
        min_chunks=max(6, max_chunks//2)
    )

    curated = expand_chunk_context(curated)
    print(f"  → After re-ranking: {len(curated)} high-relevance chunks")

    print(f"  → Chunk relevance scores:")
    for i, chunk in enumerate(curated[:5], 1):
        score = chunk.get('relevance_score', 0)
        filename = chunk.get('filename', 'unknown')[:40]
        text_preview = chunk.get('text', '')[:80].replace('\n', ' ')
        print(f"     [{i}] Score: {score:.3f} | {filename} | {text_preview}...")

    node_ctx = kg_result.get("grounding", {}).get("NodeContext", {})
    compact_nodes = kg_result.get("grounding", {}).get("Nodes", [])
    compact_edges = kg_result.get("grounding", {}).get("Edges", [])

    # BUILD SYSTEM AND USER MESSAGES (NEW APPROACH)
    system_msg, user_msg = build_grounded_messages(
        q, compact_nodes, compact_edges, node_ctx, curated,
        mode=mode, preset_params=preset_params
    )

    # Generate answer with proper system/user structure
    resp = client.responses.create(
        model=model,
        input=[
            {"role": "system", "content": system_msg},
            {"role": "user", "content": user_msg}
        ]
    )
    answer = resp.output_text

    # Apply post-processing to enhance formatting
    # answer = enhance_answer_formatting(answer, has_citations=len(curated) > 0, mode=mode)
    print(f"  ✓ Applied formatting enhancements for {mode} mode")

    # Verify grounding (keep for logging)
    verification = verify_answer_grounding(answer, curated, client)
    if verification['confidence'] < 0.25:
        print(f"  → Grounding confidence: {verification['confidence']:.2f}")

    file_to_idxs, idx_to_src = build_citation_map(curated)

    return {
        "answer": answer,
        "verification": verification,
        "subqueries": subqs,
        "curated_chunks": curated,
        "citation_index_to_source": idx_to_src,
        "files_to_citation_indices": file_to_idxs,
        "provenance_from_kg": prov,
        "model_used": model,
        "mode": mode,
        "prompt_chars": len(system_msg) + len(user_msg)
    }

"""## Export to Markdown"""

def to_superscript_anchors(answer_text, idx_to_src):
    if not answer_text:
        return answer_text or ""

    def repl(m):
        n = m.group(1)
        if str(n) in idx_to_src or int(n) in idx_to_src:
            return f'<sup><a href="#cite-{n}">[{n}]</a></sup>'
        return m.group(0)

    return re.sub(r"\[(\d+)\]", repl, answer_text)

def export_markdown(final, question, save_dir=None):
    answer = final.get("answer", "") or ""
    idx_to_src = final.get("citation_index_to_source", {}) or {}
    files_to_idxs = final.get("files_to_citation_indices", {}) or {}

    labeled_answer = to_superscript_anchors(answer, idx_to_src)
    ts = datetime.now().strftime("%Y-%m-%d %H:%M")
    title = f"KG + VectorStore Answer - {question or 'Untitled'}"

    lines = [f"# **{title}**\n", f"_Generated: {ts}_\n", "## **Answer**\n", labeled_answer + "\n", "---\n", "## **Citations**\n"]

    if idx_to_src:
        for n in sorted(idx_to_src.keys(), key=lambda x: int(x) if str(x).isdigit() else str(x)):
            src = idx_to_src.get(n) or {}
            fn = _short_filename(src.get("filename"))
            # Each citation on new line with blank line after
            lines.append(f'<a id="cite-{n}"></a>')
            lines.append(f'**[{n}]** → **{fn}**')
            lines.append("")  # Blank line after each citation
    else:
        lines.append("_No citations available._")
    lines.append("")

    if files_to_idxs:
        lines.append("---\n")
        lines.append("## **Sources by File**\n")
        for fn, idxs in sorted(files_to_idxs.items()):
            idxs_disp = ", ".join(f"[{i}]" for i in idxs)
            lines.append(f"- **{_short_filename(fn)}** → {idxs_disp}")
        lines.append("")

    md_text = "\n".join(lines)

    if not save_dir:
        save_dir = os.getenv("EKG_EXPORT_DIR") or os.path.join(os.getcwd(), "outputs")
    os.makedirs(save_dir, exist_ok=True)
    slug = _slugify(question or "answer")
    outpath = os.path.join(save_dir, f"{slug}--{datetime.now().strftime('%Y%m%d-%H%M')}.md")
    with open(outpath, "w", encoding="utf-8") as f:
        f.write(md_text)

    # if _HAS_DISPLAY:
    # else:
    #     print(md_text)
    #     print(f"\nFile: {outpath}")

    return md_text, outpath

"""## Interactive Quiz Feature"""

def generate_quiz_from_answer(answer_text, client, num_questions=5):
    """Generate multiple choice quiz from the answer"""
    system_msg = """You are a quiz generator. Create multiple choice questions to test understanding.
Return ONLY valid JSON with no additional text or markdown formatting."""

    user_msg = f"""Based on this answer about mutual funds order lifecycle, generate exactly {num_questions} multiple choice questions.

Answer text:
{answer_text[:3000]}

Return ONLY this JSON structure with no markdown code blocks:
{{
  "questions": [
    {{
      "question": "Question text here?",
      "options": ["A) First option", "B) Second option", "C) Third option", "D) Fourth option"],
      "correct": 0
    }}
  ]
}}

Rules:
- Generate exactly {num_questions} questions
- Each question must have exactly 4 options (A, B, C, D)
- The "correct" field is the index (0, 1, 2, or 3) of the correct option
- Test key concepts from the answer
- Make questions clear and unambiguous"""

    try:
        resp = client.responses.create(
            model="gpt-4o-mini",
            input=[{"role": "system", "content": system_msg},
                   {"role": "user", "content": user_msg}],
            max_output_tokens=2000,
            temperature=0.3
        )

        response_text = get_output_text(resp)
        print(f"Raw response preview: {response_text[:200]}...")

        quiz_data = safe_parse_json(response_text)

        if not quiz_data or "questions" not in quiz_data:
            print(f"⚠️ Invalid quiz data structure: {quiz_data}")
            return []

        questions = quiz_data.get("questions", [])
        print(f"✓ Parsed {len(questions)} questions successfully")
        return questions

    except Exception as e:
        print(f"❌ Error generating quiz: {type(e).__name__}: {e}")
        return []

def show_answer_with_quiz(question, mode, hybrid_result, answer_cache, client):
    """Display answer with OK/Refresh/Quiz buttons"""

    # Display the answer
    md_text, file_path = export_markdown(final=hybrid_result, question=question)

    if not _HAS_WIDGETS:
        print("\nInteractive buttons not available in this environment")
        return

    # Create buttons
    ok_button = widgets.Button(description="OK", button_style='success', icon='check')
    refresh_button = widgets.Button(description="Refresh Answer", button_style='warning', icon='refresh')
    quiz_button = widgets.Button(description="Quiz", button_style='info', icon='question')
    output = widgets.Output()

    def on_ok_clicked(b):
        with output:
            clear_output()
            print("✓ Answer accepted. Moving on...")

    def on_refresh_clicked(b):
        with output:
            clear_output()
            print("♻️  Regenerating answer with fresh data...\n")

            clear_retrieval_cache()
            params = get_preset(mode)

            kg_result = answer_with_kg(
                q=question, G=G, by_id=by_id, name_index=name_index,
                llm_client=client, preset_params=params
            )

            new_hybrid_result = hybrid_answer(
                q=question, kg_result=kg_result, by_id=by_id,
                client=client, vs_id=VECTOR_STORE_ID, preset_params=params
            )

            query_embedding = get_question_embedding(client, question)
            cache_answer(answer_cache, question, mode, new_hybrid_result, query_embedding, client)

            print("\n✓ Fresh answer generated and cached\n")
            md_text, file_path = export_markdown(final=new_hybrid_result, question=question)
            print(f"\nExport completed: {file_path}")

    def on_quiz_clicked(b):
        with output:
            clear_output()
            print("Generating quiz questions...")

            answer_text = hybrid_result.get("answer", "")

            if not answer_text.strip():
                print("No answer available.")
                return

            # Generate questions
            quiz_questions = generate_quiz_from_answer(answer_text, client, num_questions=5)
            quiz_questions = normalize_questions(quiz_questions)

            if not quiz_questions:
                print("Failed to generate questions.")
                return

            print(f"Generated {len(quiz_questions)} questions. Opening quiz...\n")

            # Create simple Gradio quiz
            with gr.Blocks(title="Knowledge Quiz") as demo:
                gr.Markdown("## Test Your Understanding")

                qs_state = gr.State(quiz_questions)
                radios = []

                for i, q in enumerate(quiz_questions):
                    gr.Markdown(f"**Q{i+1}.** {q['question']}")
                    radio = gr.Radio(choices=q['options'], label="")
                    radios.append(radio)

                submit_btn = gr.Button("Submit Quiz", variant="primary")
                result_html = gr.HTML("")

                def on_submit(qs, *answers):
                    ans_list = list(answers) + [None] * (5 - len(answers))
                    return _score_quiz(qs, *ans_list[:5])

                submit_btn.click(on_submit, inputs=[qs_state] + radios, outputs=[result_html])

            demo.launch(share=False, inline=True)

    ok_button.on_click(on_ok_clicked)
    refresh_button.on_click(on_refresh_clicked)
    quiz_button.on_click(on_quiz_clicked)

    print("\n" + "="*80)
    print("Review the answer above. Choose an option:")
    print("="*80)
    button_box = widgets.HBox([ok_button, refresh_button, quiz_button])

"""## Gradio Quiz"""

def _score_quiz(qs, a1, a2, a3, a4, a5):
    answers = [a1, a2, a3, a4, a5]
    if not qs:
        return "❌ No quiz loaded. Click “Generate Quiz” first."
    score, lines = 0, []
    for i, q in enumerate(qs):
        chosen = answers[i]
        opts   = q["options"]
        ci     = q["correct"]
        corr   = opts[ci] if 0 <= ci < len(opts) else None
        if not chosen:
            lines.append(f"Q{i+1}: <em>Not answered</em>. Correct: <b>{html.escape(str(corr))}</b>")
            continue
        if chosen == corr:
            score += 1
            lines.append(f"Q{i+1}: ✅ <b>Correct</b>")
        else:
            lines.append(
                f"Q{i+1}: ❌ Incorrect — Your: <b>{html.escape(str(chosen))}</b> | "
                f"Correct: <b>{html.escape(str(corr))}</b>"
            )
    pct = int(round((score/len(qs))*100)) if qs else 0
    tone = "Excellent! 🔥" if pct >= 80 else "Good job! 👍" if pct >= 60 else "Keep at it! 📚"
    return f"<h3>Score: {score}/{len(qs)}</h3><p>{tone}</p><ul>{''.join(f'<li>{x}</li>' for x in lines)}</ul>"

def make_quiz_app_with_questions(questions):
    """Create Gradio quiz with pre-loaded questions"""

    with gr.Blocks(title="Knowledge Quiz") as demo:
        gr.Markdown("## Test Your Understanding")
        gr.Markdown(f"Answer all {len(questions)} questions below and click Submit to see your score.")

        # Store questions in state
        qs_state = gr.State(questions)

        # Create radio buttons for each question
        radios = []
        for i, q in enumerate(questions):
            gr.Markdown(f"### Question {i+1}")
            gr.Markdown(f"{q['question']}")
            radio = gr.Radio(
                choices=q['options'],
                label=f"Select your answer for Question {i+1}:",
                interactive=True
            )
            radios.append(radio)
            gr.Markdown("---")  # Separator between questions

        with gr.Row():
            submit_btn = gr.Button("Submit Quiz", variant="primary", size="lg")

        result_html = gr.HTML("")

        def on_submit(qs, *answers):
            # Pad answers to 5 if fewer questions
            answers_list = list(answers) + [None] * (5 - len(answers))
            return _score_quiz(qs, *answers_list[:5])

        submit_btn.click(
            on_submit,
            inputs=[qs_state] + radios,
            outputs=[result_html]
        )

    return demo

# def make_quiz_blocks(generate_quiz_fn, client):
#     """
#     generate_quiz_fn: callable(answer_text, client, num_questions=5) -> list[dict]
#     Returns a gr.Blocks app.
#     """
#     with gr.Blocks(title="KG + Vector Quiz") as demo:
#         gr.Markdown("### Quick Check Quiz\nGenerate questions from your synthesized answer and test yourself.")
#         answer_box = gr.Textbox(
#             label="Source Answer (from your KG + Vector pipeline)",
#             lines=8,
#             placeholder="Paste the synthesized answer here..."
#         )

#         gen_btn = gr.Button("Generate Quiz", variant="primary")
#         qs_state = gr.State([])

#         # Five question slots
#         q_labels = [gr.Markdown(visible=False) for _ in range(5)]
#         radios   = [gr.Radio([], label="", interactive=True, visible=False) for _ in range(5)]

#         with gr.Row():
#             submit_btn = gr.Button("Submit Quiz", variant="primary")
#             reset_btn  = gr.Button("Reset")

#         result_html = gr.HTML("")

#         # Generate quiz handler
#         def on_gen(answer_text):
#             if not answer_text.strip():
#                 return [gr.update()] * 16  # Return no changes

#             qs = normalize_questions(generate_quiz_fn(answer_text, client, num_questions=5))

#             if not qs:
#                 return [gr.update()] * 16

#             labels = [f"**Q{i+1}.** {q['question']}" for i, q in enumerate(qs)]
#             options = [q["options"] for q in qs]

#             # Pad to 5
#             while len(labels) < 5:
#                 labels.append("")
#                 options.append([])

#             # Visibility flags
#             vis = [bool(lbl) for lbl in labels]

#             return (
#                 [qs] +                           # state
#                 labels +                         # 5 labels
#                 options +                        # 5 options arrays
#                 vis + vis +                      # show labels & radios
#                 [gr.update(value="", visible=True)]  # clear result
#             )

#         gen_btn.click(
#             on_gen,
#             inputs=[answer_box],
#             outputs=[qs_state,
#                      q_labels[0], q_labels[1], q_labels[2], q_labels[3], q_labels[4],
#                      radios[0],   radios[1],   radios[2],   radios[3],   radios[4],
#                      q_labels[0], q_labels[1], q_labels[2], q_labels[3], q_labels[4],
#                      radios[0],   radios[1],   radios[2],   radios[3],   radios[4],
#                      result_html]
#         )

#         # Submit quiz handler
#         def on_submit(qs, a1, a2, a3, a4, a5):
#             return _score_quiz(qs, a1, a2, a3, a4, a5)

#         submit_btn.click(
#             on_submit,
#             inputs=[qs_state] + radios,
#             outputs=[result_html]
#         )

#         # Reset handler
#         def on_reset():
#             empty_lbls = [""]*5
#             empty_opts = [[]]*5
#             hide = [False]*5
#             return (
#                 [[]] +                # state
#                 empty_lbls +          # labels
#                 empty_opts  +         # options
#                 hide + hide +         # visibility off
#                 [gr.update(value="", visible=True)]
#             )

#         reset_btn.click(
#             on_reset,
#             inputs=[],
#             outputs=[qs_state,
#                      q_labels[0], q_labels[1], q_labels[2], q_labels[3], q_labels[4],
#                      radios[0],   radios[1],   radios[2],   radios[3],   radios[4],
#                      q_labels[0], q_labels[1], q_labels[2], q_labels[3], q_labels[4],
#                      radios[0],   radios[1],   radios[2],   radios[3],   radios[4],
#                      result_html]
#         )

#     return demo

"""
Note:
The following repository originally included a notebook-exported block that attempted to
load a knowledge graph and construct a global `G`, `by_id`, and `name_index` at import time.
That behavior is inappropriate for a library module and breaks API startup when those files
or constants (e.g., BASE_FOLDER) are not present. The execution block has been removed to
avoid side effects on import. The API layer is responsible for loading artifacts on demand
and passing them into the functions defined here.
"""

def load_kg_from_json(kg_path: str):
    """
    Load knowledge graph from JSON file. This is the original loading logic
    from the notebook, extracted into a reusable function.
    
    Returns: (G, by_id, name_index) tuple
    """
    with open(kg_path, "r", encoding="utf-8") as f:
        KG = json.load(f)
    
    nodes = KG.get("nodes", [])
    edges = KG.get("edges", [])
    
    # Build by_id index
    by_id = {}
    for n in nodes:
        nid = n["id"]
        by_id[nid] = n
    
    # Build name_index
    name_index = defaultdict(list)
    for n in nodes:
        nid = n["id"]
        names = [n.get("name", "")] + n.get("aliases", [])
        for t in names:
            if t:
                name_index[_norm(t)].append(nid)
    
    # Build NetworkX graph
    G = nx.MultiDiGraph()
    for n in nodes:
        G.add_node(n["id"], **n)
    
    for e in edges:
        u, v = e["source_id"], e["target_id"]
        attrs = e.get("properties", {}).copy() if "properties" in e else {}
        attrs["type"] = e.get("type", "RELATED")
        G.add_edge(u, v, **attrs)
    
    return G, by_id, dict(name_index)