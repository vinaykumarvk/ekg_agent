# -*- coding: utf-8 -*-
"""kg_vector_response_v1.ipynb

Automatically generated by Colab.

Original file is located at

# Setup

## Setup: Mount Drive and Install Dependencies
"""



"""## Imports"""

try:
    import gradio as gr
    _HAS_GRADIO = True
except ImportError:
    _HAS_GRADIO = False
    gr = None

import html
import json
import re
import threading
import os
import textwrap
from datetime import datetime
from collections import defaultdict, Counter, deque
from typing import Dict, Any, List, Tuple, Iterable, Set
from concurrent.futures import ThreadPoolExecutor, as_completed
import pickle
import numpy as np
from pathlib import Path

import networkx as nx
from openai import OpenAI

try:
    from rapidfuzz import process, fuzz
    _HAS_RAPIDFUZZ = True
except ImportError:
    _HAS_RAPIDFUZZ = False
    import logging
    log = logging.getLogger(__name__)
    log.warning("rapidfuzz not available, using fallback")

try:
    from IPython.display import display, Markdown
    _HAS_DISPLAY = True
except ImportError:
    _HAS_DISPLAY = False

try:
    import ipywidgets as widgets
    from IPython.display import display, clear_output
    _HAS_WIDGETS = True
except ImportError:
    _HAS_WIDGETS = False
    import logging
    log = logging.getLogger(__name__)
    log.warning("ipywidgets not available")

"""## Answer Caching System"""

# Cache configuration - Production ready
import time
from collections import OrderedDict
from typing import Optional

# Use settings for cache configuration
try:
    from api.settings import settings
    CACHE_DIR = settings.CACHE_DIR
    MAX_CACHE_SIZE = settings.MAX_CACHE_SIZE
    CACHE_TTL = settings.CACHE_TTL
except ImportError:
    # Fallback if settings not available
    CACHE_DIR = os.getenv("CACHE_DIR", "/tmp/ekg_cache")
    MAX_CACHE_SIZE = int(os.getenv("MAX_CACHE_SIZE", "1000"))
    CACHE_TTL = int(os.getenv("CACHE_TTL", "3600"))

os.makedirs(CACHE_DIR, exist_ok=True)
ANSWER_CACHE_FILE = os.path.join(CACHE_DIR, "answer_cache.pkl")
SIMILARITY_THRESHOLD = 0.80

class LRUCache:
    """Thread-safe LRU cache with TTL support"""
    def __init__(self, max_size: int = None, ttl: int = None):
        self.max_size = max_size or MAX_CACHE_SIZE
        self.ttl = ttl or CACHE_TTL
        self.cache = OrderedDict()
        self._lock = threading.Lock()
    
    def get(self, key: str) -> Optional[Any]:
        with self._lock:
            if key in self.cache:
                value, timestamp = self.cache[key]
                if time.time() - timestamp < self.ttl:
                    self.cache.move_to_end(key)
                    return value
                else:
                    del self.cache[key]
            return None
    
    def set(self, key: str, value: Any) -> None:
        with self._lock:
            if len(self.cache) >= self.max_size:
                self.cache.popitem(last=False)
            self.cache[key] = (value, time.time())
    
    def clear(self) -> None:
        with self._lock:
            self.cache.clear()
    
    def size(self) -> int:
        with self._lock:
            return len(self.cache)

_EMBED_CACHE_LOCK = threading.Lock()
_EMBED_CACHE_MAX = 512
_EMBED_CACHE = OrderedDict()


def _cache_embedding(key, value):
    with _EMBED_CACHE_LOCK:
        if key in _EMBED_CACHE:
            _EMBED_CACHE.move_to_end(key)
        _EMBED_CACHE[key] = value
        while len(_EMBED_CACHE) > _EMBED_CACHE_MAX:
            _EMBED_CACHE.popitem(last=False)


def _get_cached_embedding(key):
    with _EMBED_CACHE_LOCK:
        cached = _EMBED_CACHE.get(key)
        if cached is None:
            return None
        _EMBED_CACHE.move_to_end(key)
        return cached


def get_embeddings(client, texts, model="text-embedding-3-small"):
    """Return embeddings for one or many texts with basic memoization."""
    if isinstance(texts, str):
        single = True
        texts = [texts]
    else:
        single = False

    results = [None] * len(texts)
    to_fetch = []
    fetch_indices = []

    for idx, text in enumerate(texts):
        key = (model, text)
        cached = _get_cached_embedding(key)
        if cached is not None:
            results[idx] = cached
        else:
            fetch_indices.append(idx)
            to_fetch.append(text)

    if to_fetch:
        response = client.embeddings.create(model=model, input=to_fetch)
        for idx, data in zip(fetch_indices, response.data):
            embedding = np.array(data.embedding, dtype=np.float32)
            key = (model, texts[idx])
            _cache_embedding(key, embedding)
            results[idx] = embedding

    return results[0] if single else results


def get_question_embedding(client, question):
    """Get embedding for semantic similarity"""
    return get_embeddings(client, question)

def cosine_similarity(a, b):
    """Calculate cosine similarity between two vectors"""
    return np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b))

def load_answer_cache():
    """Load cached answers from disk"""
    if Path(ANSWER_CACHE_FILE).exists():
        with open(ANSWER_CACHE_FILE, 'rb') as f:
            cache = pickle.load(f)
        print(f"✓ Loaded {len(cache)} cached answers")
        return cache
    return {}

def save_answer_cache(answer_cache):
    """Save answer cache to disk"""
    Path(CACHE_DIR).mkdir(parents=True, exist_ok=True)
    with open(ANSWER_CACHE_FILE, 'wb') as f:
        pickle.dump(answer_cache, f)
    print(f"✓ Saved {len(answer_cache)} answers to cache")

def find_similar_cached_answer(client, question, mode, answer_cache, threshold=SIMILARITY_THRESHOLD):
    """
    Find cached answer for semantically similar question with same mode
    Returns: (cache_key, cached_data, similarity_score) or (None, None, 0)
    """
    if not answer_cache:
        return None, None, 0

    query_embedding = get_question_embedding(client, question)

    best_match = None
    best_similarity = 0
    best_key = None

    for cache_key, cached_data in answer_cache.items():
        # Must match mode exactly
        if cached_data.get("mode") != mode:
            continue

        cached_embedding = cached_data.get("embedding")
        if cached_embedding is None:
            continue

        # Convert list back to numpy array
        cached_embedding = np.array(cached_embedding)
        similarity = cosine_similarity(query_embedding, cached_embedding)

        if similarity > best_similarity:
            best_similarity = similarity
            best_match = cached_data
            best_key = cache_key

    if best_similarity >= threshold:
        return best_key, best_match, best_similarity

    return None, None, best_similarity

def cache_answer(answer_cache, question, mode, result, embedding, client):
    """Store answer with metadata and embedding"""
    import hashlib
    cache_key = hashlib.md5(f"{question}|{mode}".encode()).hexdigest()

    answer_cache[cache_key] = {
        "question": question,
        "mode": mode,
        "result": result,
        "embedding": embedding.tolist(),  # Convert numpy to list for pickle
        "timestamp": datetime.now().isoformat(),
        "model": result.get("model_used", "gpt-4o")
    }

    # Save immediately after adding
    save_answer_cache(answer_cache)
    return cache_key

"""## Cache Management Utilities"""

def view_cache_contents():
    """Display all cached questions"""
    cache = load_answer_cache()
    if not cache:
        print("Cache is empty")
        return

    print(f"\nCached Answers ({len(cache)} total):")
    print("="*80)
    for i, (key, data) in enumerate(cache.items(), 1):
        print(f"{i}. [{data['mode'].upper()}] {data['question']}")
        print(f"   Cached: {data['timestamp']}")
        print(f"   Model: {data.get('model', 'unknown')}")
        print()

def clear_answer_cache():
    """Clear all cached answers"""
    if Path(ANSWER_CACHE_FILE).exists():
        os.remove(ANSWER_CACHE_FILE)
        print("✓ Answer cache cleared")
    else:
        print("Cache file doesn't exist")

def clear_retrieval_cache():
    """
    DEPRECATED: Retrieval cache removed - file_search tool handles caching internally.
    This function is kept for backward compatibility but does nothing.
    """
    import logging
    log = logging.getLogger(__name__)
    log.warning("clear_retrieval_cache() is deprecated - file_search tool handles caching")

"""## Configuration Presets"""

ANSWER_PRESETS = {
    "concise": {
        "hops": 1,
        "k_suggest": 12,
        "k_candidates": 15,
        "max_chunks": 6,
        "k_each": 2,
        "lambda_div": 0.6,
        "max_tokens": 1500,
        "max_expanded": 40,
        "model": "gpt-5-nano",
        "_mode": "concise",
        "chunk_char_limit": 900,
        "chunk_sentence_limit": 6,
        "max_subqueries": 6,
        "retrieval_workers": 4,
        "min_chunks": 4
    },
    "balanced": {
        "hops": 1,
        "k_suggest": 20,
        "k_candidates": 25,
        "max_chunks": 10,
        "k_each": 3,
        "lambda_div": 0.6,
        "max_tokens": 6000,
        "max_expanded": 60,
        "model": "gpt-5.1",
        "_mode": "balanced",
        "chunk_char_limit": 1400,
        "chunk_sentence_limit": 10,
        "max_subqueries": 8,
        "retrieval_workers": 6,
        "min_chunks": 6
    },
    "deep": {
        "hops": 2,
        "k_suggest": 35,
        "k_candidates": 40,
        "max_chunks": 22,
        "k_each": 8,
        "lambda_div": 0.75,
        "max_tokens": 20000,
        "max_expanded": 120,
        "model": "o3-deep-research",
        "_mode": "deep",
        "chunk_char_limit": 2200,
        "chunk_sentence_limit": 16,
        "max_subqueries": 10,
        "retrieval_workers": 8,
        "min_chunks": 8
    }
}

def get_preset(mode="balanced"):
    mode = mode.lower()
    if mode not in ANSWER_PRESETS:
        print(f"Warning: Unknown mode '{mode}', using 'balanced'")
        mode = "balanced"
    return ANSWER_PRESETS[mode]

"""## Prompt Templates (PROMPT_SET)

Centralized prompt templates for flexible prompt management.
Modify prompts here without touching code logic.
"""

file_search_message = """
You are an internal wealth-management product knowledge assistant.

You have access to the following:
1. A structured set of business concepts, entities, and relationships derived from our internal knowledge graph:
<KG_CONTEXT>
{kg_text}
</KG_CONTEXT>

2. A document vector store containing detailed product, process, and journey information.

The user has provided several questions and hints that together express ONE underlying information need.
Treat ALL of the following as different perspectives of the same broader inquiry:

<USER_QUESTIONS>
{expanded_queries_str}
</USER_QUESTIONS>

------------------------------------------------------------
YOUR TASK
------------------------------------------------------------

1) **Understand the Unified Intent**
   Infer the single business problem or information objective these questions collectively express.
   Reformulate this as a clear, concise intent statement.

2) **Use the KG Context Internally**
   Use the entities and relationships in <KG_CONTEXT> ONLY for internal reasoning—
   NOT for display, NOT for enumeration, NOT for explanation.
   They exist only to guide your understanding of concepts and their interactions.

3) **Retrieve the Most Relevant Knowledge**
   Internally issue file_search queries into the document vector store by combining:
      • the unified intent
      • the expanded understanding of the topic
      • relevant concepts implied by the KG context
   Do NOT mention file_search, retrieval, vector stores, or any internal mechanisms in your final answer.

4) **Synthesize ONE Integrated Explanation**
   Using the retrieved content:
      • Produce a cohesive, structured narrative that directly answers the unified intent.
      • Merge overlapping points and remove redundancies.
      • Ensure the explanation is generic and platform-agnostic.
      • Do NOT reference clients, banks, proprietary implementations, or example organizations.
      • Do NOT mention knowledge graphs, nodes, edges, triples, metadata, or relationships.
      • Do NOT start the answer with statements like "Below is an integrated view…",
        "The KG shows…", "From the knowledge graph…", or any similar framing.
      • Simply present the explanation directly and professionally, as if preparing internal
        product documentation or briefing an experienced BA.

5) **Final Output Format (Strict)**
Respond ONLY with the following JSON object:

{{
  "stepback_intent": "A clear statement of the unified intent.",
  "expanded_question": "A concise, enriched reformulation.",
  "business_entities": ["entity 1", "entity 2", "..."],
  "answer": "A single integrated answer written in **proper, clean Markdown**, with headings, subheadings, bullet points, or numbered lists.\n             The answer must NOT include the KG, nodes, relationships, internal tools, or any meta-commentary."
}}

Notes:
- The **answer** field must contain fully valid Markdown **inside the JSON** (no backticks).
- Do NOT add any prose outside the JSON.
- Do NOT confirm the usage of KG or documents.
- Do NOT reference the prompt, instructions, or tools in the answer.
"""

stepback_message = """
You are a wealth-management product knowledge assistant.

You ALWAYS receive one user question, and you MUST analyse it without asking the user to repeat it.

You have access to a file_search tool connected to a KG vector store
containing natural-language descriptions of platform knowledge-graph nodes.

===========================================================
YOUR MANDATORY TASKS
===========================================================

For the given user question (provided in the `original_question` field below):

1) STEP-BACK INTENT
   Rewrite the question in a more generic, clarified formulation that captures
   the underlying business intent. No loss of meaning. No assumptions.

2) EXPANDED QUESTION
   Expand the question into a richer, more detailed version intended to improve
   retrieval from the KG. Keep it concise, business-focused, and not rhetorical.

3) EXTRACT BUSINESS ENTITIES
   Extract key BUSINESS entities mentioned or implied in the expanded question.
   Examples: workflows, screens, documents, validations, client data entities,
   approval processes, reports, regulatory aspects, system modules, etc.
   Output as a list of short canonical phrases.

4) RETRIEVE RELEVANT KG NODES
   - You MUST call the file_search tool.
   - Build your search query using a combination of:
        a) the expanded question
        b) the extracted entities (as keywords)
   - Retrieve up to 10 (not necessarily exactly 10) relevant KG nodes.
   - From each result, extract ONLY the KG node NAME (not description).

5) OUTPUT FORMAT (STRICT)
   You MUST output ONLY the following JSON object:

   {{
     "original_question": "...",
     "stepback_question": "...",
     "expanded_question": "...",
     "entities": ["...", "..."],
     "node_names": ["...", "..."]
   }}

   - All string fields may contain markdown formatting.
   - "entities" must be an array of strings.
   - "node_names" must be an array of strings. If no matches, return [].

===========================================================
STRICT RULES
===========================================================

- DO NOT ask the user to provide the question again.
- DO NOT output explanations, commentary, or extra text.
- DO NOT refuse unless the input is empty.
- DO NOT repeat tool call results; only extract node names.
- DO NOT invent node names; use only those returned by file_search.

===========================================================
INPUT TO PROCESS
===========================================================

original_question: {question}
"""

formatting_message = """
You are a senior editor specializing in transforming research reports into
clear, polished, publication-ready documents.

Your task is to refine the following research output:

<RAW_ANSWER>
{answer}
</RAW_ANSWER>

------------------------------------------------------------
INSTRUCTIONS
------------------------------------------------------------

1. **Improve Language & Clarity**
   - Rewrite the content to be clearer, sharper, and more professional.
   - Improve flow, transitions, and readability without changing factual meaning.
   - Remove repetition and tighten overly long phrasing.

2. **Improve Structure & Formatting**
   - Apply clean Markdown formatting.
   - Use headings, subheadings, numbered lists, and bullet points where helpful.
   - Ensure the document reads like a polished research brief or executive summary.

3. **Handle References Correctly**
   - Remove all inline references, markers, or citation tags.
   - Collect all references mentioned anywhere in the text.
   - Present them in a **single consolidated "References" section** at the end.
   - If a reference appears multiple times, list it only once.

4. **Preserve Meaning, Remove Noise**
   - Do NOT hallucinate any new facts.
   - Preserve technical accuracy, definitions, and logical structure.
   - Eliminate filler language or unnecessary qualifiers.

5. **Output Format**
   - Return ONLY the refined research report as a **Markdown document**.
   - No extra commentary, no explanation of changes.
"""

PROMPT_SET = {
    "concise": file_search_message,
    "balanced": file_search_message,
    "deep": file_search_message,
    "stepback": stepback_message,
    "formatting": formatting_message,
}

"""# Initialise functions

## Utility Functions
"""

def _norm(s):
    return re.sub(r"\s+", " ", (s or "").strip().lower())

def _dedup(seq, key=lambda x: x):
    seen, out = set(), []
    for x in seq:
        k = key(x)
        if k not in seen:
            seen.add(k)
            out.append(x)
    return out

def _extract_json_block(text):
    m = re.search(r"```json\s*(\{.*?\})\s*```", text, flags=re.S)
    if m:
        return m.group(1)
    s, e = text.find("{"), text.rfind("}")
    if s != -1 and e != -1 and e > s:
        cand = text[s:e+1]
        if cand.count("{") == cand.count("}"):
            return cand
    return None

def safe_parse_json(block: str):
    """Extract and parse the first JSON object from model output safely."""
    s = block.strip()
    s = re.sub(r"^```(?:json)?\s*|\s*```$", "", s)   # remove ```json fences
    m = re.search(r"\{.*\}", s, flags=re.S)           # find JSON object
    if not m:
        return {}
    try:
        return json.loads(m.group(0))
    except json.JSONDecodeError:
        return {}

def get_output_text(resp):
    if hasattr(resp, "output_text") and isinstance(resp.output_text, str):
        return resp.output_text
    try:
        parts = []
        for item in getattr(resp, "output", []) or []:
            content = getattr(item, "content", None) or item.get("content", None)
            if not content:
                continue
            for seg in content:
                txt = getattr(seg, "text", None) or seg.get("text", None)
                if txt:
                    parts.append(txt)
        if parts:
            return "\n".join(parts)
    except Exception:
        pass
    return str(resp) if resp is not None else ""

def parse_llm_json(text):
    """
    Robust JSON parsing from LLM output.
    Handles markdown fences, control characters, and various edge cases.
    """
    # 1. Strip markdown fences if present
    cleaned_text = text.strip()
    if cleaned_text.startswith("```json"):
        cleaned_text = cleaned_text[len("```json"):].strip()
    if cleaned_text.endswith("```"):
        cleaned_text = cleaned_text[:-len("```")].strip()

    try:
        return json.loads(cleaned_text)
    except json.JSONDecodeError as e:
        import logging
        log = logging.getLogger(__name__)
        log.warning(f"Initial JSONDecodeError: {e}. Attempting further cleanup.")
        # 2. Try to remove invalid control characters (0x00-0x1F, 0x7F) that are not allowed in JSON strings
        # This will remove characters like vertical tab (\x0b), form feed (\x0c) etc.
        cleaned_text_stage2 = re.sub(r'[\x00-\x1f\x7f]', '', cleaned_text)
        try:
            return json.loads(cleaned_text_stage2)
        except json.JSONDecodeError as e2:
            log.warning(f"Second JSONDecodeError after control character removal: {e2}. Could not parse LLM response.")
            raise

def get_response(message, client, model, vector_ids, system_message=None, stream=False):
    """
    Wrapper for OpenAI Responses API with file_search tool support.
    
    Args:
        message: User message content
        client: OpenAI client instance
        model: Model name
        vector_ids: List of vector store IDs for file_search
        system_message: Optional system message (if None, only user message is sent)
        stream: Whether to stream the response
    
    Returns:
        Response object from OpenAI API
    """
    log = logging.getLogger(__name__)
    input_msgs = []
    if system_message:
        input_msgs.append({"role": "system", "content": system_message})
    input_msgs.append({"role": "user", "content": message})

    # Log the outbound payload (redacted content) for debugging failures
    try:
        log.debug(
            "OpenAI request: model=%s stream=%s vector_store_ids=%s user_chars=%s",
            model,
            stream,
            vector_ids if isinstance(vector_ids, list) else [vector_ids],
            len(message or ""),
        )
    except Exception:
        pass

    try:
        response = client.responses.create(
            model=model,
            input=input_msgs,
            tools=[
                {
                    "type": "file_search",
                    "vector_store_ids": vector_ids if isinstance(vector_ids, list) else [vector_ids]
                }
            ],
            stream=stream
        )
        return response
    except Exception as e:
        log.error(
            "OpenAI request failed: model=%s stream=%s vector_store_ids=%s error=%s",
            model,
            stream,
            vector_ids if isinstance(vector_ids, list) else [vector_ids],
            e,
        )
        raise

def _slugify(text, max_len=80):
    text = (text or "answer").strip().lower()
    text = re.sub(r"[^\w\s-]", "", text)
    text = re.sub(r"\s+", "-", text).strip("-")
    return (text or "answer")[:max_len]

def _short_filename(name, maxlen=64):
    if not name:
        return "unknown-file"
    base = os.path.basename(name)
    return (base[:maxlen-1] + "...") if len(base) > maxlen else base

def normalize_questions(qs):
    """Ensure list of {question:str, options:list[str], correct:int} (max 5)."""
    good = []
    for q in (qs or []):
        if not isinstance(q, dict):
            continue
        if not all(k in q for k in ("question","options","correct")):
            continue
        opts = [str(o).strip() for o in (q.get("options") or [])]
        if len(opts) < 2 or not isinstance(q.get("correct"), int):
            continue
        good.append({
            "question": str(q["question"]).strip(),
            "options": opts,
            "correct": q["correct"]
        })
    return good[:5]

def indent(text, pad="  "):
    """Indent text block"""
    return "\n".join(pad + ln for ln in (text or "").splitlines())

def analyze_answer_formatting(answer_text, mode="balanced"):
    """
    Analyze and report formatting metrics for evaluation scores.
    This helps verify that enhancements are working correctly.

    Args:
        answer_text: The answer to analyze
        mode: The mode used (for comparison with expectations)

    Returns:
        Dictionary with counts and estimated scores
    """
    import re

    if not answer_text:
        print("No answer text to analyze")
        return None

    # Count formatting elements
    citations = len(re.findall(r'\[(?:\d+|KG:[^\]]+)\]', answer_text))

    bullets = answer_text.count('•') + answer_text.count('\n- ') + answer_text.count('\n* ')

    markers = len(re.findall(
        r'(?:Key Point|Note|Important|Example|Reasoning|Evidence|Analysis|Insight|Key Insight|Overview|Summary):',
        answer_text
    ))

    # Count colons for explainability
    colons = answer_text.count(':')

    # Calculate sentence stats
    sentences = len(re.findall(r'[.!?]+\s', answer_text))
    words = len(answer_text.split())
    avg_sentence_length = words / sentences if sentences > 0 else 0

    # Calculate estimated scores using the evaluation formulas
    # Attribution = 3 + (2/1.2) * citation_count, clipped to 1-5
    attribution = min(5.0, max(1.0, 3 + (2/1.2) * citations))

    # Explainability = 3 + 0.25 * markers + 0.3 * bullets, clipped to 1-5
    explainability = min(5.0, max(1.0, 3 + 0.25 * markers + 0.3 * bullets))

    # Clarity = 4 - 0.05 * max(0, mean_words - 30) + 0.2 * bullets, clipped to 1-5
    clarity = 4 - 0.05 * max(0, avg_sentence_length - 30) + 0.2 * bullets
    clarity = min(5.0, max(1.0, clarity))

    # Mode-specific targets
    targets = {
        "concise": {"citations": "1-2", "bullets": "3-5", "markers": "1-2"},
        "balanced": {"citations": "2-3", "bullets": "5-10", "markers": "3-5"},
        "deep": {"citations": "3-5", "bullets": "8-15", "markers": "5-8"}
    }
    target = targets.get(mode, targets["balanced"])

    # Print analysis
    print(f"\n{'='*70}")
    print(f"ANSWER FORMATTING ANALYSIS - Mode: {mode.upper()}")
    print(f"{'='*70}")

    print(f"\nCOUNTS:")
    print(f"  Citations:     {citations:2d}  (target: {target['citations']})")
    print(f"  Bullets:       {bullets:2d}  (target: {target['bullets']})")
    print(f"  Markers:       {markers:2d}  (target: {target['markers']})")
    print(f"  Colons:        {colons:2d}")

    print(f"\nCLARITY METRICS:")
    print(f"  Total words:   {words}")
    print(f"  Sentences:     {sentences}")
    print(f"  Avg words/sentence: {avg_sentence_length:.1f} (target: <30)")

    print(f"\nESTIMATED SCORES:")
    print(f"  Attribution:    {attribution:.2f} / 5.00")
    print(f"  Explainability: {explainability:.2f} / 5.00")
    print(f"  Clarity:        {clarity:.2f} / 5.00")

    # Overall assessment
    avg_score = (attribution + explainability + clarity) / 3
    print(f"\n  Average:        {avg_score:.2f} / 5.00")

    if avg_score >= 4.5:
        print(f"  Assessment:     ✓ EXCELLENT")
    elif avg_score >= 4.0:
        print(f"  Assessment:     ✓ GOOD")
    elif avg_score >= 3.5:
        print(f"  Assessment:     ~ ADEQUATE")
    else:
        print(f"  Assessment:     ⚠ NEEDS IMPROVEMENT")

    print(f"{'='*70}\n")

    return {
        "counts": {
            "citations": citations,
            "bullets": bullets,
            "markers": markers,
            "colons": colons,
            "sentences": sentences,
            "words": words
        },
        "metrics": {
            "avg_sentence_length": avg_sentence_length
        },
        "scores": {
            "attribution": attribution,
            "explainability": explainability,
            "clarity": clarity,
            "average": avg_score
        }
    }

"""## Interactive Answer Review"""

def show_answer_with_review(question, mode, hybrid_result, answer_cache, client):
    """Display answer with OK/Refresh buttons"""

    # Display the answer
    md_text, file_path = export_markdown(final=hybrid_result, question=question)

    if not _HAS_WIDGETS:
        print("\nInteractive buttons not available in this environment")
        return

    # Create buttons
    ok_button = widgets.Button(description="OK", button_style='success', icon='check')
    refresh_button = widgets.Button(description="Refresh Answer", button_style='warning', icon='refresh')
    output = widgets.Output()

    def on_ok_clicked(b):
        with output:
            clear_output()
            print("✓ Answer accepted. Moving on...")

    def on_refresh_clicked(b):
        with output:
            clear_output()
            print("♻️  Regenerating answer with fresh data...\n")

            # Clear retrieval cache and regenerate
            clear_retrieval_cache()
            params = get_preset(mode)

            new_hybrid_result = answer_with_kg_and_vector(
                q=question,
                G=G,
                by_id=by_id,
                name_index=name_index,
                client=client,
                kg_vector_store_id=VECTOR_STORE_ID,
                doc_vector_store_id=VECTOR_STORE_ID,
                preset_params=params
            )

            # Cache the new answer
            query_embedding = get_question_embedding(client, question)
            cache_answer(answer_cache, question, mode, new_hybrid_result, query_embedding, client)

            print("\n✓ Fresh answer generated and cached\n")

            # Display new answer
            md_text, file_path = export_markdown(final=new_hybrid_result, question=question)
            print(f"\nExport completed: {file_path}")

    ok_button.on_click(on_ok_clicked)
    refresh_button.on_click(on_refresh_clicked)

    # Display buttons and output
    print("\n" + "="*80)
    print("Review the answer above. Choose an option:")
    print("="*80)
    button_box = widgets.HBox([ok_button, refresh_button])

"""## New KG Workflow Functions (Using file_search tool)"""

def map_node_names_to_ids(
    node_names: List[str],
    by_id: Dict[str, Dict],
    name_index: Dict[str, Any] = None
) -> List[str]:
    """
    Map node names returned by GPT to actual node IDs in the graph.

    Args:
        node_names: List of node names from GPT (e.g., ["customer card", "product selection screen"])
        by_id: Your existing node lookup dict {node_id: node_data}
        name_index: Optional name-to-ID index for faster lookup

    Returns:
        List of node IDs that exist in the graph
    """
    node_ids = []
    not_found = []

    for node_name in node_names:
        node_name_lower = _norm(node_name)

        # Strategy 1: Use name_index if available (fastest)
        if name_index:
            # Try exact match
            if node_name_lower in name_index:
                ids = name_index[node_name_lower]
                if isinstance(ids, (list, tuple)):
                    node_ids.extend([str(id) for id in ids])
                else:
                    node_ids.append(str(ids))
                continue

        # Strategy 2: Search through by_id (fallback)
        found = False
        for node_id, node_data in by_id.items():
            # Get node name from data
            actual_name = _norm(
                node_data.get('name') or
                node_data.get('node_name') or
                node_data.get('label') or
                str(node_id)
            )

            # Check for match
            if node_name_lower == actual_name or node_name_lower in actual_name or actual_name in node_name_lower:
                node_ids.append(str(node_id))
                found = True
                break

        if not found:
            not_found.append(node_name)

    # Report not found nodes (optional logging)
    if not_found:
        import logging
        log = logging.getLogger(__name__)
        log.debug(f"Could not find {len(not_found)} nodes in graph: {not_found[:5]}")

    # Deduplicate
    return list(set(node_ids))


def get_relevant_nodes(
    question: str,
    kg_vector_store_id: str,
    client,
    model: str = "gpt-4o",
    max_nodes: int = 10
) -> Dict[str, Any]:
    """
    Use GPT with file_search to get relevant nodes, entities, and stepback.

    Args:
        question: User question
        kg_vector_store_id: Vector store ID for KG nodes
        client: OpenAI client
        model: Model name
        max_nodes: Maximum nodes to retrieve

    Returns:
        {
            "original_question": "...",
            "stepback_question": "...",
            "expanded_question": "...",
            "entities": ["...", "..."],
            "node_names": ["...", "..."]
        }
    """
    message = PROMPT_SET["stepback"].format(question=question)
    vector_ids = [kg_vector_store_id] if isinstance(kg_vector_store_id, str) else kg_vector_store_id

    try:
        resp = get_response(message, client, model, vector_ids)
    except Exception as e:
        import logging
        log = logging.getLogger(__name__)
        log.error(f"Error in response from OpenAI - get_relevant_nodes: {e}")
        # Fallback: return minimal structure
        return {
            "original_question": question,
            "stepback_question": question,
            "expanded_question": question,
            "entities": [],
            "node_names": []
        }

    # Parse response
    output_text = get_output_text(resp)

    # Use robust JSON parsing
    try:
        result = parse_llm_json(output_text)
        return result
    except (json.JSONDecodeError, Exception) as e:
        import logging
        log = logging.getLogger(__name__)
        log.warning(f"Could not parse JSON from get_relevant_nodes: {e}")
        log.debug(f"Raw output: {output_text[:500]}")

        # Fallback: return minimal structure
        return {
            "original_question": question,
            "stepback_question": question,
            "expanded_question": question,
            "entities": [],
            "node_names": []
        }


def get_relevant_subgraph(
    question: str,
    G,
    by_id: Dict[str, Dict],
    kg_vector_store_id: str,
    client,
    stepback_response: Dict,
    name_index: Dict[str, Any] = None,
    edge_type_whitelist: List[str] = None,
    hops: int = 1,
    max_expanded: int = 60
) -> Dict[str, Any]:
    """
    Complete new workflow: GPT file search + graph expansion.

    Replaces the old 4-step process.

    Args:
        question: User question
        G: NetworkX graph
        by_id: Node lookup dict
        kg_vector_store_id: Vector store ID for KG
        client: OpenAI client
        stepback_response: Result from get_relevant_nodes
        name_index: Optional name-to-ID index
        edge_type_whitelist: Optional edge type filter
        hops: Graph expansion hops
        max_expanded: Maximum nodes to expand

    Returns:
        {
            "question_analysis": {...},  # Stepback, entities, etc.
            "seed_node_ids": [...],       # Starting nodes
            "expanded_node_ids": [...],   # All nodes in subgraph
            "edges": [...],               # Edges in subgraph
            "nodes": {}                   # Node data for subgraph
        }
    """
    analysis = stepback_response

    # STEP 2: Map node names to IDs
    seed_node_ids = map_node_names_to_ids(
        node_names=analysis.get('node_names', []),
        by_id=by_id,
        name_index=name_index
    )

    if not seed_node_ids:
        return {
            "question_analysis": analysis,
            "seed_node_ids": [],
            "expanded_node_ids": [],
            "edges": [],
            "nodes": {}
        }

    # STEP 3: Expand graph from seed nodes
    expanded_node_ids, edges = expand_nodes(
        G=G,
        seed_ids=seed_node_ids,
        hops=hops,
        edge_type_whitelist=edge_type_whitelist,
        max_expanded=max_expanded
    )

    # STEP 4: Get node data for expanded nodes
    nodes = {}
    for node_id in expanded_node_ids:
        if node_id in by_id:
            nodes[node_id] = by_id[node_id]

    return {
        "question_analysis": analysis,
        "seed_node_ids": seed_node_ids,
        "expanded_node_ids": expanded_node_ids,
        "edges": edges,
        "nodes": nodes
    }


def build_kg_guided_queries(
    question: str,
    kg_result: Dict[str, Any],
    by_id: Dict[str, Dict],
    max_queries: int = 12
) -> List[str]:
    """Build KG-guided queries for vector retrieval."""
    queries = []

    # Get data from kg_result
    expanded_node_ids = kg_result.get("expanded_node_ids", [])
    edges = kg_result.get("edges", [])
    analysis = kg_result.get("question_analysis", {})

    # 1. Original question
    queries.append(question)

    # 2. Stepback/expanded
    if "stepback_question" in analysis:
        queries.append(analysis["stepback_question"])
    if "expanded_question" in analysis:
        queries.append(analysis["expanded_question"])

    # 3. Entity-focused
    node_names = []
    for node_id in expanded_node_ids[:8]:
        if node_id in by_id:
            name = by_id[node_id].get('name', by_id[node_id].get('node_name', str(node_id)))
            node_names.append(name)

    for entity in node_names[:5]:
        queries.append(f"{question} {entity}")

    # 4. Relationship-focused
    node_lookup = {}
    for node_id in expanded_node_ids:
        if node_id in by_id:
            name = by_id[node_id].get('name', by_id[node_id].get('node_name', str(node_id)))
            node_lookup[node_id] = name

    for edge in edges[:10]:
        source_name = node_lookup.get(edge.get("source_id"), "")
        target_name = node_lookup.get(edge.get("target_id"), "")
        edge_type = edge.get("type", "RELATED")

        if source_name and target_name:
            queries.append(f"{source_name} {edge_type} {target_name}")

    # 5. Deduplicate
    seen = set()
    unique_queries = []
    for q in queries:
        q_lower = _norm(q)
        if q_lower and q_lower not in seen:
            seen.add(q_lower)
            unique_queries.append(q)

    return unique_queries[:max_queries]


def generate_kg_text(
    kg_result: Dict[str, Any],
    by_id: Dict[str, Dict],
) -> str:
    """
    Generate formatted KG text for prompt context.

    Args:
        kg_result: Result from get_relevant_subgraph
        by_id: Node lookup dict

    Returns:
        Formatted KG text string
    """
    expanded_node_ids = kg_result.get("expanded_node_ids", [])
    edges = kg_result.get("edges", [])
    analysis = kg_result.get("question_analysis", {})

    # Build compact node list
    nodes_summary = []
    for node_id in expanded_node_ids[:40]:  # Limit to 40 nodes
        if node_id in by_id:
            name = by_id[node_id].get('name', by_id[node_id].get('node_name', str(node_id)))
            node_type = by_id[node_id].get('node_type', by_id[node_id].get('type', 'Entity'))
            nodes_summary.append(f"• {name} ({node_type})")

    # Build compact edge list
    node_lookup = {}
    for node_id in expanded_node_ids:
        if node_id in by_id:
            name = by_id[node_id].get('name', by_id[node_id].get('node_name', str(node_id)))
            node_lookup[node_id] = name

    edges_summary = []
    for edge in edges[:50]:  # Limit to 50 edges
        source = node_lookup.get(edge.get("source_id"), "?")
        target = node_lookup.get(edge.get("target_id"), "?")
        rel_type = edge.get("type", "RELATED")
        edges_summary.append(f"• {source} --[{rel_type}]→ {target}")

    kg_structure_text = f"""
The following entities and relationships are relevant to your question:

ENTITIES:
{chr(10).join(nodes_summary)}

RELATIONSHIPS:
{chr(10).join(edges_summary)}

This structure shows WHAT entities exist and HOW they relate.
Use this to understand the architecture and connections.
The actual detailed documentation will be retrieved via file_search.
"""
    return kg_structure_text.strip()


def stream_response(message, client, model, vector_ids, system_message=None):
    """
    Stream a response from the Responses API including tool calls.
    Handles:
      - output_text.delta
      - tool_call events
      - tool_result events
    """
    # Always stream here – this function is for streaming usage
    iterator = get_response(message, client, model, vector_ids, system_message=system_message, stream=True)

    full_text = ""

    for event in iterator:
        # Safely convert event to dict
        data = event.model_dump() if hasattr(event, 'model_dump') else dict(event) if isinstance(event, dict) else {}
        etype = data.get("type", "")

        # -----------------------------
        # 1. TEXT CHUNKS (main streaming)
        # -----------------------------
        if "output_text.delta" in etype or "delta" in etype:
            delta = data.get("delta", "")
            if isinstance(delta, dict):
                chunk = delta.get("text", "")
            elif isinstance(delta, str):
                chunk = delta
            else:
                chunk = ""

            if chunk:
                print(chunk, end="", flush=True)
                full_text += chunk

        # -----------------------------
        # 2. TOOL CALL START
        # -----------------------------
        elif "tool_call" in etype:
            # Optional: print something or ignore
            tool_type = data.get("tool", {}).get("type") if isinstance(data.get("tool"), dict) else None
            # You can comment this out if you don't want to see tool events
            pass

        # -----------------------------
        # 3. TOOL RESULTS RETURNING
        # -----------------------------
        elif "tool_result" in etype:
            # Optional: hide, print, or log tool results
            pass

        # -----------------------------
        # 4. COMPLETION EVENT
        # -----------------------------
        elif "completed" in etype or "done" in etype:
            # Optional: final marker
            pass

    print()  # newline at end
    return full_text

"""## Step 1: Entity Suggestion (Legacy - kept for backward compatibility)"""

def stepback_entity_candidates(q, llm_client, model="gpt-4o", k_suggest=20, tone="executive"):
    system = "You are an analyst for a Wealth Management application. Respond ONLY with JSON: {\"entities\": [...]}. No extra text."
    user = f"""Question: {q}

Return ONLY: {{"entities": ["name1", "name2", ...]}}
Entities should be concise names from a wealth/order-management knowledge graph.
Tone: {tone}"""

    resp = llm_client.responses.create(
        model=model,
        input=[{"role": "system", "content": system}, {"role": "user", "content": user}],
        max_output_tokens=400,
        temperature=0.2
    )
    data = safe_parse_json(get_output_text(resp)) or {}
    ents = [e.strip() for e in data.get("entities", []) if isinstance(e, str) and e.strip()]

    out, seen = [], set()
    for e in ents:
        n = _norm(e)
        if n and n not in seen:
            seen.add(n)
            out.append(e)
    return out[:k_suggest]

"""## Step 2: Entity Matching"""

def fuzzy_candidate_search(suggested, name_index, max_aliases_per_suggest=5, fuzzy_threshold=80):
    aliases = list(name_index.keys())
    out_aliases = []

    if _HAS_RAPIDFUZZ:
        for s in suggested:
            s_norm = _norm(s)
            if not s_norm:
                continue
            scored = process.extract(s_norm, aliases, scorer=fuzz.token_set_ratio, limit=min(len(aliases), max_aliases_per_suggest * 3))
            picked = [alias for alias, score, _ in scored if score >= fuzzy_threshold]
            out_aliases.extend(picked[:max_aliases_per_suggest])
    else:
        alias_norm_map = {alias: _norm(alias) for alias in aliases}
        for s in suggested:
            s_norm = _norm(s)
            if not s_norm:
                continue
            tokens = [t for t in re.split(r"[^a-z0-9]+", s_norm) if t]
            def score(alias):
                a = alias_norm_map[alias]
                return sum(t in a or a in t for t in tokens)
            ranked = sorted(aliases, key=score, reverse=True)
            picked = [a for a in ranked if score(a) > 0][:max_aliases_per_suggest]
            out_aliases.extend(picked)

    return _dedup(out_aliases, key=lambda x: x.lower())

def resolve_aliases_deterministic(candidate_aliases, name_index, by_id, k_candidates=25):
    matched = []
    for alias in candidate_aliases:
        ids = name_index.get(alias)
        if ids is None:
            continue
        if isinstance(ids, (list, tuple)):
            for nid in ids:
                nid = str(nid)
                if nid in by_id:
                    matched.append((nid, alias))
        else:
            nid = str(ids)
            if nid in by_id:
                matched.append((nid, alias))
    return _dedup(matched, key=lambda x: x[0])[:k_candidates]

def match_entities_to_graph(suggested, name_index, by_id, max_aliases=5, threshold=80, k_candidates=25):
    candidates = fuzzy_candidate_search(suggested, name_index, max_aliases, threshold)
    return resolve_aliases_deterministic(candidates, name_index, by_id, k_candidates)

"""## Step 3: Graph Expansion"""

def expand_nodes(G, seed_ids, hops=1, edge_type_whitelist=None, max_expanded=60, preset_params=None):
    if preset_params:
        max_expanded = preset_params.get("max_expanded", max_expanded)

    def _ok_edge(edata):
        if not edge_type_whitelist:
            return True
        et = edata.get("type") or edata.get("label") or "RELATED"
        return et in edge_type_whitelist

    seen = set(seed_ids)
    q = deque([(nid, 0) for nid in seed_ids])
    edges = []

    while q and len(seen) < max_expanded:
        u, d = q.popleft()
        if u not in G:
            continue

        for v in G[u]:
            for _, edata in G[u][v].items():
                if not _ok_edge(edata):
                    continue
                et = edata.get("type") or edata.get("label") or "RELATED"
                edge = {"source_id": u, "target_id": v, "type": str(et)}
                if "evidence" in edata:
                    edge["evidence"] = edata["evidence"]
                if "source_documents" in edata:
                    edge["source_documents"] = edata["source_documents"]
                edges.append(edge)
            if v not in seen and d < hops:
                seen.add(v)
                q.append((v, d + 1))

        if hasattr(G, "predecessors"):
            for v in G.predecessors(u):
                for _, edata in G[v][u].items():
                    if not _ok_edge(edata):
                        continue
                    et = edata.get("type") or edata.get("label") or "RELATED"
                    edge = {"source_id": v, "target_id": u, "type": str(et)}
                    if "evidence" in edata:
                        edge["evidence"] = edata["evidence"]
                    if "source_documents" in edata:
                        edge["source_documents"] = edata["source_documents"]
                    edges.append(edge)
                if v not in seen and d < hops:
                    seen.add(v)
                    q.append((v, d + 1))

    edges = _dedup(edges, key=lambda e: (e["source_id"], e["target_id"], e["type"]))
    return list(seen), edges

"""## Evidence Collection"""

def collect_edge_evidence(edges, nodes):
    evidence_map = defaultdict(list)
    if not edges:
        return {}
    allowed = set(nodes) if nodes else None

    for e in edges:
        if isinstance(e, dict):
            src = e.get("source_id") or e.get("source")
            dst = e.get("target_id") or e.get("target")
            rel = e.get("type") or "related_to"
            attrs = e
        else:
            continue

        if allowed and (src not in allowed or dst not in allowed):
            continue

        ev = attrs.get("evidence") or attrs.get("snippet") or ""
        prov = {k: attrs.get(k) for k in ("doc_id", "doc_name", "file", "page") if k in attrs}
        key = f"{src}-{rel}-{dst}"
        evidence_map[key].append({"source": src, "target": dst, "type": rel, "evidence": ev, **prov})

    return dict(evidence_map)

def node_context_from_evidence(edge_evidence, by_id):
    edge_evidence = edge_evidence or {}
    by_id = by_id or {}

    def _name(nid):
        meta = by_id.get(nid, {})
        return meta.get("name") or meta.get("label") or str(nid) if isinstance(meta, dict) else str(nid)

    per_node = defaultdict(list)
    for key, ev_list in edge_evidence.items():
        try:
            src, rel, dst = key.split('-', 2)
        except ValueError:
            continue

        src_name, dst_name = _name(src), _name(dst)
        for ev in ev_list or []:
            snippet = ev.get("evidence") or ""
            line = f"{src_name} --({rel})-> {dst_name}"
            if snippet:
                line += f": {snippet}"
            per_node[src].append(line)
            per_node[dst].append(line)

    return {nid: "\n".join(lines) for nid, lines in per_node.items()}

"""## Generate Answer from KG"""



def grounded_answer_llm(q, nodes, edges, by_id, llm_client, model="gpt-4o", max_tokens=3000, preset_params=None):
    """Generate answer from KG context only with enhanced formatting"""
    if preset_params:
        max_tokens = preset_params.get("max_tokens", max_tokens)
        model = preset_params.get("model", model)
        mode = preset_params.get("_mode", "balanced")
    else:
        mode = "balanced"

    def _fmt_node(n):
        if isinstance(n, dict):
            return {"id": n.get("id"), "name": n.get("name"), "type": n.get("type")}
        meta = by_id.get(n, {})
        return {"id": n, "name": meta.get("name") or str(n), "type": meta.get("type")}

    def _fmt_edge(e):
        if isinstance(e, dict):
            return {"source": e.get("source_id"), "target": e.get("target_id"), "type": e.get("type")}
        return {"source": str(e), "target": str(e), "type": "related"}

    compact_nodes = [_fmt_node(n) for n in (nodes or [])]
    compact_edges = [_fmt_edge(e) for e in (edges or [])]

    edge_evidence = collect_edge_evidence(edges or [], nodes or [])
    node_context = node_context_from_evidence(edge_evidence, by_id)

    # Mode-specific system messages with formatting requirements
    if mode == "concise":
        system_msg = (
            "You are a precise wealth-management assistant. "
            "Use ONLY the provided graph context.\n\n"
            "CRITICAL FORMATTING REQUIREMENTS:\n"
            "1. ATTRIBUTION: Include citations [1], [2] for key claims\n"
            "2. STRUCTURE: Use clean bullet points (•) for key information\n"
            "3. CLARITY: Maximum 20 words per sentence\n"
            "4. LENGTH: Keep total answer under 250 words\n"
            "5. PROFESSIONAL: Write like a business document for analysts\n\n"
            "ANSWER TEMPLATE:\n"
            "**Answer:** [2-5 sentence summary] [1]\n\n"
            "**Key Information:**\n"
            "• [brief detail] [2]\n"
            "• [brief detail]\n"
            "• [brief detail] [1]\n\n"
            "**Important:** [Any important clarification]\n\n"
            "Every factual statement should have a citation. "
        )
    elif mode == "deep":
        system_msg = (
            "You are a precise wealth-management assistant. "
            "Use ONLY the provided graph context.\n\n"
            "CRITICAL FORMATTING REQUIREMENTS:\n"
            "1. ATTRIBUTION:\n"
            "   - Include citations using [1], [2], [3], [4], [5]\n"
            "   - Also cite KG entities as [KG: entity_name]\n"
            "   - Cite EVERY factual statement, rule, number, or condition\n"
            "2. STRUCTURE:\n"
            "   - Use extensive bullet points (•) for all lists and findings\n"
            "   - Use sub-bullets (  -) for hierarchical details\n"
            "   - Create 5-8 numbered sections for complex topics\n"
            "   - Each section should have 3-6 bullet points\n"
            "3. MARKERS:\n"
            "   - Use professional markers: 'Important:', 'Note:', 'Example:', 'Analysis:', 'Evidence:'\n"
            "   - Add 3-5 markers throughout the answer for clarity\n"
            "4. CLARITY:\n"
            "   - Maximum 25 words per sentence\n"
            "   - One concept per sentence\n"
            "   - Short paragraphs (2-3 sentences max between bullets)\n"
            "5. COMPLETENESS:\n"
            "   - Answer should be 1000-2500 words for complex topics\n"
            "   - Cover ALL details from sources\n"
            "   - Include ALL rules, conditions, and exceptions mentioned\n\n"
            "ANSWER STRUCTURE:\n"
            "**Overview:** [3-5 sentence definition] [1]\n\n"
            "**1. Main Topic**\n"
            "Important: [Introduction] [2]\n"
            "• [detailed explanation] [3]\n"
            "  - Sub-detail A\n"
            "  - Sub-detail B\n"
            "• [detailed explanation] [KG: entity] [4]\n\n"
            "**2. Second Topic**\n"
            "Note: [Key insight]\n"
            "• [comprehensive detail] [1]\n"
            "• [comprehensive detail] [5]\n\n"
            "[Continue with 3-6 more sections following this pattern]\n\n"
            "**Key Insights:**\n"
            "• Critical takeaway 1\n"
            "• Critical takeaway 2\n\n"
            "Follow this pattern. Cite EVERY factual claim. "
            "Include ALL details from sources. "
        )
    else:  # balanced
        system_msg = (
            "You are a precise wealth-management assistant. "
            "Use ONLY the provided graph context.\n\n"
            "CRITICAL FORMATTING REQUIREMENTS:\n"
            "1. ATTRIBUTION: Include citations [1], [2], [3] for key claims\n"
            "2. STRUCTURE: Use clean bullet points (•) for key information\n"
            "3. CLARITY: Maximum 25 words per sentence\n"
            "4. PROFESSIONAL: Write like a business document for analysts\n"
            "5. MARKERS: Use 'Important:', 'Note:', 'Example:' sparingly and professionally\n\n"
            "ANSWER TEMPLATE:\n"
            "**Overview:** [3-5 sentence summary] [1]\n\n"
            "**Key Information:**\n"
            "• [detailed explanation] [2]\n"
            "• [detailed explanation] [3]\n"
            "• [detailed explanation] [1]\n\n"
            "**Important:** [Any important clarification]\n\n"
        )

    # Mode-specific user message templates
    if mode == "concise":
        user_msg = f"""## Question
{q}

## KG Nodes
{json.dumps(compact_nodes[:30], ensure_ascii=False)}

## KG Edges
{json.dumps(compact_edges[:40], ensure_ascii=False)}

Structure your answer:
**Answer:** [1-2 sentences with citation]

**Key Information:**
• [brief detail] [KG: entity]
• [brief detail]
• [brief detail]

Use evidence provided. Do not invent."""

    elif mode == "deep":
        user_msg = f"""## Question
{q}

## KG Nodes
{json.dumps(compact_nodes[:30], ensure_ascii=False)}

## KG Edges
{json.dumps(compact_edges[:40], ensure_ascii=False)}

Structure your answer comprehensively:

**Overview:** [Brief definition with citation]

**Main Information:**
• [detailed explanation] [KG: entity]
  - Sub-detail A
  - Sub-detail B
• [detailed explanation] [KG: entity]
• [detailed explanation]

**Key Insights:**
• Insight 1
• Insight 2

**Important:** [Any clarifications]

Use ALL evidence provided. Be thorough. Do not invent."""

    else:  # balanced
        user_msg = f"""## Question
{q}

## KG Nodes
{json.dumps(compact_nodes[:30], ensure_ascii=False)}

## KG Edges
{json.dumps(compact_edges[:40], ensure_ascii=False)}

Structure your answer:
**Answer:** [Brief overview with citation]

**Key Information:**
• [explanation] [KG: entity]
• [explanation]
• [explanation] [KG: entity]

**Important:** [Any key notes]

Use evidence provided. Do not invent."""

    try:
        resp = llm_client.responses.create(
            model=model,
            input=[{"role": "system", "content": system_msg}, {"role": "user", "content": user_msg}],
            max_output_tokens=max_tokens
        )
        answer_text = getattr(resp, "output_text", "") or ""
    except Exception as e:
        answer_text = f"_Error: {type(e).__name__}_"

    # Apply post-processing
    # answer_text = enhance_answer_formatting(answer_text, has_citations=False, mode=mode)

    return {
        "answer": answer_text,
        "grounding": {"Question": q, "Nodes": compact_nodes, "Edges": compact_edges, "NodeContext": node_context},
        "meta": {"model": model, "nodes": len(compact_nodes), "edges": len(compact_edges), "mode": mode}
    }

"""## Complete KG Pipeline"""

def answer_with_kg(q, G, by_id, name_index, llm_client, hops=1, k_suggest=20, k_candidates=25, preset_params=None):
    if preset_params:
        hops = preset_params.get("hops", hops)
        k_suggest = preset_params.get("k_suggest", k_suggest)
        k_candidates = preset_params.get("k_candidates", k_candidates)

    suggested = stepback_entity_candidates(q, llm_client, k_suggest=k_suggest)
    matched = match_entities_to_graph(suggested, name_index, by_id, k_candidates=k_candidates)
    seed_ids = [nid for nid, _ in matched]

    if not seed_ids:
        return {
            "answer": "_No entities matched in the knowledge graph._",
            "resolved_entities": [],
            "supporting_edges": [],
            "grounding": {},
            "meta": {"reason": "No seeds", "suggested": suggested}
        }

    expanded_nodes, expanded_edges = expand_nodes(G, seed_ids, hops=hops, max_expanded=60, preset_params=preset_params)
    data = grounded_answer_llm(q, expanded_nodes, expanded_edges, by_id, llm_client, preset_params=preset_params)

    data["meta"].update({
        "suggested_entities": suggested,
        "seed_ids": seed_ids,
        "expanded_nodes": len(expanded_nodes),
        "expanded_edges": len(expanded_edges)
    })
    data["resolved_entities"] = expanded_nodes
    data["supporting_edges"] = expanded_edges
    return data

def kg_anchors(resolved_entities, supporting_edges, by_id):
    """Extract entity names and provenance from KG results"""
    ent_names = []
    for n in resolved_entities or []:
        if isinstance(n, dict):
            ent_names.append(n.get("name") or n.get("label") or n.get("id"))
        else:
            meta = (by_id or {}).get(n, {}) if isinstance(by_id, dict) else {}
            ent_names.append(meta.get("name") or meta.get("label") or str(n))
    ent_names = [x for x in ent_names if x]
    ent_names = list(dict.fromkeys(ent_names))  # Deduplicate

    provenance = []
    for e in supporting_edges or []:
        attrs = None
        if isinstance(e, dict):
            attrs = e
        elif isinstance(e, (list, tuple)) and len(e) >= 3 and isinstance(e[2], dict):
            attrs = e[2]
        if attrs:
            prov = {
                k: attrs.get(k)
                for k in ("doc_id", "doc_name", "file", "page", "source", "module", "market")
                if attrs.get(k) is not None
            }
            if prov:
                provenance.append(prov)
    return ent_names, provenance


def answer_with_kg_and_vector(q, G, by_id, name_index, client, kg_vector_store_id, doc_vector_store_id, preset_params=None):
    """
    New workflow: Complete KG + Vector pipeline using file_search tool.
    
    This replaces the old hybrid_answer function.
    
    Args:
        q: Question
        G: NetworkX graph
        by_id: Node lookup dict
        name_index: Name index
        client: OpenAI client
        kg_vector_store_id: Vector store ID for KG nodes
        doc_vector_store_id: Vector store ID for documents
        preset_params: Preset parameters
    
    Returns:
        Dict with answer, citations, etc.
    """
    if preset_params:
        model = preset_params.get("model", "gpt-4o")
        mode = preset_params.get("_mode", "balanced")
        hops = preset_params.get("hops", 1)
        max_expanded = preset_params.get("max_expanded", 60)
    else:
        model = "gpt-4o"
        mode = "balanced"
        hops = 1
        max_expanded = 60
    
    # Step 1: Get relevant nodes using file_search
    stepback_response = get_relevant_nodes(
        question=q,
        kg_vector_store_id=kg_vector_store_id,
        client=client,
        model=model,
        max_nodes=10
    )
    
    # Step 2: Get relevant subgraph
    kg_result = get_relevant_subgraph(
        question=q,
        G=G,
        by_id=by_id,
        kg_vector_store_id=kg_vector_store_id,
        client=client,
        stepback_response=stepback_response,
        name_index=name_index,
        hops=hops,
        max_expanded=max_expanded
    )
    
    # Step 3: Build KG-guided queries
    expanded_queries = build_kg_guided_queries(
        question=q,
        kg_result=kg_result,
        by_id=by_id,
        max_queries=12
    )
    
    # Step 4: Generate KG text
    kg_text = generate_kg_text(kg_result, by_id)
    
    # Step 5: Format prompt
    expanded_queries_str = "\n".join(f"{i+1}. {query}" for i, query in enumerate(expanded_queries))
    message = PROMPT_SET.get(mode, PROMPT_SET["balanced"]).format(
        expanded_queries_str=expanded_queries_str,
        kg_text=kg_text
    )
    
    # Step 6: Get response with file_search tool
    resp = get_response(
        message=message,
        client=client,
        model=model,
        vector_ids=[doc_vector_store_id]
    )
    
    # Step 7: Parse response
    output_text = get_output_text(resp)
    try:
        answer_json = parse_llm_json(output_text)
        answer = answer_json.get('answer', output_text)
    except (json.JSONDecodeError, Exception):
        answer = output_text
    
    # Build provenance from KG
    ents, prov = kg_anchors(
        kg_result.get("expanded_node_ids", []),
        kg_result.get("edges", []),
        by_id
    )

    return {
        "answer": answer,
        "subqueries": expanded_queries,
        "curated_chunks": [],  # file_search handles this internally
        "citation_index_to_source": {},  # Citations handled by tool
        "files_to_citation_indices": {},
        "provenance_from_kg": prov,
        "model_used": model,
        "mode": mode,
        "prompt_chars": len(message),
        "kg_result": kg_result  # Include KG result for reference
    }



"""## Export to Markdown"""

def to_superscript_anchors(answer_text, idx_to_src):
    if not answer_text:
        return answer_text or ""

    def repl(m):
        n = m.group(1)
        if str(n) in idx_to_src or int(n) in idx_to_src:
            return f'<sup><a href="#cite-{n}">[{n}]</a></sup>'
        return m.group(0)

    return re.sub(r"\[(\d+)\]", repl, answer_text)

def export_markdown(final, question, save_dir=None):
    answer = final.get("answer", "") or ""
    idx_to_src = final.get("citation_index_to_source", {}) or {}
    files_to_idxs = final.get("files_to_citation_indices", {}) or {}

    labeled_answer = to_superscript_anchors(answer, idx_to_src)
    ts = datetime.now().strftime("%Y-%m-%d %H:%M")
    title = f"KG + VectorStore Answer - {question or 'Untitled'}"

    lines = [f"# **{title}**\n", f"_Generated: {ts}_\n", "## **Answer**\n", labeled_answer + "\n", "---\n", "## **Citations**\n"]

    if idx_to_src:
        for n in sorted(idx_to_src.keys(), key=lambda x: int(x) if str(x).isdigit() else str(x)):
            src = idx_to_src.get(n) or {}
            fn = _short_filename(src.get("filename"))
            # Each citation on new line with blank line after
            lines.append(f'<a id="cite-{n}"></a>')
            lines.append(f'**[{n}]** → **{fn}**')
            lines.append("")  # Blank line after each citation
    else:
        lines.append("_No citations available._")
    lines.append("")

    if files_to_idxs:
        lines.append("---\n")
        lines.append("## **Sources by File**\n")
        for fn, idxs in sorted(files_to_idxs.items()):
            idxs_disp = ", ".join(f"[{i}]" for i in idxs)
            lines.append(f"- **{_short_filename(fn)}** → {idxs_disp}")
        lines.append("")

    md_text = "\n".join(lines)

    if not save_dir:
        save_dir = os.getenv("EKG_EXPORT_DIR") or os.path.join(os.getcwd(), "outputs")
    os.makedirs(save_dir, exist_ok=True)
    slug = _slugify(question or "answer")
    outpath = os.path.join(save_dir, f"{slug}--{datetime.now().strftime('%Y%m%d-%H%M')}.md")
    with open(outpath, "w", encoding="utf-8") as f:
        f.write(md_text)

    # if _HAS_DISPLAY:
    # else:
    #     print(md_text)
    #     print(f"\nFile: {outpath}")

    return md_text, outpath

"""## Interactive Quiz Feature"""

def generate_quiz_from_answer(answer_text, client, num_questions=5):
    """Generate multiple choice quiz from the answer"""
    system_msg = """You are a quiz generator. Create multiple choice questions to test understanding.
Return ONLY valid JSON with no additional text or markdown formatting."""

    user_msg = f"""Based on this answer about mutual funds order lifecycle, generate exactly {num_questions} multiple choice questions.

Answer text:
{answer_text[:3000]}

Return ONLY this JSON structure with no markdown code blocks:
{{
  "questions": [
    {{
      "question": "Question text here?",
      "options": ["A) First option", "B) Second option", "C) Third option", "D) Fourth option"],
      "correct": 0
    }}
  ]
}}

Rules:
- Generate exactly {num_questions} questions
- Each question must have exactly 4 options (A, B, C, D)
- The "correct" field is the index (0, 1, 2, or 3) of the correct option
- Test key concepts from the answer
- Make questions clear and unambiguous"""

    try:
        resp = client.responses.create(
            model="gpt-4o-mini",
            input=[{"role": "system", "content": system_msg},
                   {"role": "user", "content": user_msg}],
            max_output_tokens=2000,
            temperature=0.3
        )

        response_text = get_output_text(resp)
        print(f"Raw response preview: {response_text[:200]}...")

        quiz_data = safe_parse_json(response_text)

        if not quiz_data or "questions" not in quiz_data:
            print(f"⚠️ Invalid quiz data structure: {quiz_data}")
            return []

        questions = quiz_data.get("questions", [])
        print(f"✓ Parsed {len(questions)} questions successfully")
        return questions

    except Exception as e:
        print(f"❌ Error generating quiz: {type(e).__name__}: {e}")
        return []

def show_answer_with_quiz(question, mode, hybrid_result, answer_cache, client):
    """Display answer with OK/Refresh/Quiz buttons"""

    # Display the answer
    md_text, file_path = export_markdown(final=hybrid_result, question=question)

    if not _HAS_WIDGETS:
        print("\nInteractive buttons not available in this environment")
        return

    # Create buttons
    ok_button = widgets.Button(description="OK", button_style='success', icon='check')
    refresh_button = widgets.Button(description="Refresh Answer", button_style='warning', icon='refresh')
    quiz_button = widgets.Button(description="Quiz", button_style='info', icon='question')
    output = widgets.Output()

    def on_ok_clicked(b):
        with output:
            clear_output()
            print("✓ Answer accepted. Moving on...")

    def on_refresh_clicked(b):
        with output:
            clear_output()
            print("♻️  Regenerating answer with fresh data...\n")

            clear_retrieval_cache()
            params = get_preset(mode)

            new_hybrid_result = answer_with_kg_and_vector(
                q=question,
                G=G,
                by_id=by_id,
                name_index=name_index,
                client=client,
                kg_vector_store_id=VECTOR_STORE_ID,
                doc_vector_store_id=VECTOR_STORE_ID,
                preset_params=params
            )

            query_embedding = get_question_embedding(client, question)
            cache_answer(answer_cache, question, mode, new_hybrid_result, query_embedding, client)

            print("\n✓ Fresh answer generated and cached\n")
            md_text, file_path = export_markdown(final=new_hybrid_result, question=question)
            print(f"\nExport completed: {file_path}")

    def on_quiz_clicked(b):
        with output:
            clear_output()
            print("Generating quiz questions...")

            answer_text = hybrid_result.get("answer", "")

            if not answer_text.strip():
                print("No answer available.")
                return

            # Generate questions
            quiz_questions = generate_quiz_from_answer(answer_text, client, num_questions=5)
            quiz_questions = normalize_questions(quiz_questions)

            if not quiz_questions:
                print("Failed to generate questions.")
                return

            print(f"Generated {len(quiz_questions)} questions. Opening quiz...\n")

            # Create simple Gradio quiz
            with gr.Blocks(title="Knowledge Quiz") as demo:
                gr.Markdown("## Test Your Understanding")

                qs_state = gr.State(quiz_questions)
                radios = []

                for i, q in enumerate(quiz_questions):
                    gr.Markdown(f"**Q{i+1}.** {q['question']}")
                    radio = gr.Radio(choices=q['options'], label="")
                    radios.append(radio)

                submit_btn = gr.Button("Submit Quiz", variant="primary")
                result_html = gr.HTML("")

                def on_submit(qs, *answers):
                    ans_list = list(answers) + [None] * (5 - len(answers))
                    return _score_quiz(qs, *ans_list[:5])

                submit_btn.click(on_submit, inputs=[qs_state] + radios, outputs=[result_html])

            demo.launch(share=False, inline=True)

    ok_button.on_click(on_ok_clicked)
    refresh_button.on_click(on_refresh_clicked)
    quiz_button.on_click(on_quiz_clicked)

    print("\n" + "="*80)
    print("Review the answer above. Choose an option:")
    print("="*80)
    button_box = widgets.HBox([ok_button, refresh_button, quiz_button])

"""## Gradio Quiz"""

def _score_quiz(qs, a1, a2, a3, a4, a5):
    answers = [a1, a2, a3, a4, a5]
    if not qs:
        return "❌ No quiz loaded. Click “Generate Quiz” first."
    score, lines = 0, []
    for i, q in enumerate(qs):
        chosen = answers[i]
        opts   = q["options"]
        ci     = q["correct"]
        corr   = opts[ci] if 0 <= ci < len(opts) else None
        if not chosen:
            lines.append(f"Q{i+1}: <em>Not answered</em>. Correct: <b>{html.escape(str(corr))}</b>")
            continue
        if chosen == corr:
            score += 1
            lines.append(f"Q{i+1}: ✅ <b>Correct</b>")
        else:
            lines.append(
                f"Q{i+1}: ❌ Incorrect — Your: <b>{html.escape(str(chosen))}</b> | "
                f"Correct: <b>{html.escape(str(corr))}</b>"
            )
    pct = int(round((score/len(qs))*100)) if qs else 0
    tone = "Excellent! 🔥" if pct >= 80 else "Good job! 👍" if pct >= 60 else "Keep at it! 📚"
    return f"<h3>Score: {score}/{len(qs)}</h3><p>{tone}</p><ul>{''.join(f'<li>{x}</li>' for x in lines)}</ul>"

def make_quiz_app_with_questions(questions):
    """Create Gradio quiz with pre-loaded questions"""

    with gr.Blocks(title="Knowledge Quiz") as demo:
        gr.Markdown("## Test Your Understanding")
        gr.Markdown(f"Answer all {len(questions)} questions below and click Submit to see your score.")

        # Store questions in state
        qs_state = gr.State(questions)

        # Create radio buttons for each question
        radios = []
        for i, q in enumerate(questions):
            gr.Markdown(f"### Question {i+1}")
            gr.Markdown(f"{q['question']}")
            radio = gr.Radio(
                choices=q['options'],
                label=f"Select your answer for Question {i+1}:",
                interactive=True
            )
            radios.append(radio)
            gr.Markdown("---")  # Separator between questions

        with gr.Row():
            submit_btn = gr.Button("Submit Quiz", variant="primary", size="lg")

        result_html = gr.HTML("")

        def on_submit(qs, *answers):
            # Pad answers to 5 if fewer questions
            answers_list = list(answers) + [None] * (5 - len(answers))
            return _score_quiz(qs, *answers_list[:5])

        submit_btn.click(
            on_submit,
            inputs=[qs_state] + radios,
            outputs=[result_html]
        )

    return demo

"""
Note:
The following repository originally included a notebook-exported block that attempted to
load a knowledge graph and construct a global `G`, `by_id`, and `name_index` at import time.
That behavior is inappropriate for a library module and breaks API startup when those files
or constants (e.g., BASE_FOLDER) are not present. The execution block has been removed to
avoid side effects on import. The API layer is responsible for loading artifacts on demand
and passing them into the functions defined here.
"""

def load_kg_from_json(kg_path: str):
    """
    Load knowledge graph from JSON file. This is the original loading logic
    from the notebook, extracted into a reusable function.
    
    Returns: (G, by_id, name_index) tuple
    """
    with open(kg_path, "r", encoding="utf-8") as f:
        KG = json.load(f)
    
    nodes = KG.get("nodes", [])
    edges = KG.get("edges", [])
    
    # Build by_id index
    by_id = {}
    for n in nodes:
        nid = n["id"]
        by_id[nid] = n
    
    # Build name_index
    name_index = defaultdict(list)
    for n in nodes:
        nid = n["id"]
        names = [n.get("name", "")] + n.get("aliases", [])
        for t in names:
            if t:
                name_index[_norm(t)].append(nid)
    
    # Build NetworkX graph
    G = nx.MultiDiGraph()
    for n in nodes:
        G.add_node(n["id"], **n)
    
    for e in edges:
        u, v = e["source_id"], e["target_id"]
        attrs = e.get("properties", {}).copy() if "properties" in e else {}
        attrs["type"] = e.get("type", "RELATED")
        G.add_edge(u, v, **attrs)
    
    return G, by_id, dict(name_index)
